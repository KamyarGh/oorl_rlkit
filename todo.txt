Figure out why things are running so slowly!!!!!!!
    - First check if pixel rendering without saving is the culprit or saving the buffer or loading it

Parallel Data Gathering & Training:
    - MPI stuff

Refactor:
    - REALLY need to refactor this code at some point to make the 1) DMCS and 2) the meta-learning
        stuff not possible to have stupid random bugs inside

Evaluation Scripts:
    - Write a script that takes the save directory and the specific exp_specs variant and evaluates it
        and generates pixels, then saves the pixels to a directory so that you can see if the model is
        actually working or not more easily

Getting Meta-IRL to Work:
    - (done) Implement dmcs for meta-rl algorithms
        - (done already) Need a new type of replay buffer where we keep track of the task-defining parameters
    - Implement the meta-reacher environment
        - (done) Implement a meta-environment for DMCS
    - (done) YOU WOULD MAYBE WANT TO ADD NEW OBJECTS ETC. TO THE PHYSICS ENGINE SO YOU WOULD NEED
        TO DO THIS IN THE ENVIRONMENT NOT THE TASK. HENCE JUST FINISH THE WAY YOU WERE DOING IT NOW!
    - (done) Run meta-reacher experiment
        - make sure that the shaped rewards are correct

    - Implement script for taking a trained model checkpoint and generating expert trajectories for:
        - Non meta IRL
        - Meta IRL

    - Initial Meta-IRL experiments:
        - Train a GAIL for simple meta-reacher but not meta version, i.e. concatenate the task params to the
            input of the policy that is being trained with GAIL
        - Train the meta-learning version where you just encoder the last K timesteps (K small, concat them, pass
            to an MLP)

    - !!!!!! Before you do this you need to write the data loader for meta-expert replay buffer
        so that you know what format to save the expert demonstrations in !!!!!!
    - !!!!!! Write a script for generating train/test splits for expert trajs !!!!!!
        - You need a script that first generates a set of train and test task params
        - Then per train and test param generates two sets of trajectories for train and test

    - Implement meta-irl algorithms
        - And for makings things run a lot faster for expert data generation, first train the expert
            with the priviledge information, then once done training load the expert and THEN generate
            the pixels. This will make things run so much faster
        - Need meta-train and meta-test splits for expert demonstrations
        - Since expert demonstrations do not change during training, instead of using the replay buffer
            implementation, use a pytorch data-loader to make things run a lot faster
        - Implement a new type of expert replay buffer from which you can:
            - Sample K tasks
            - And from each task get however many trajectories you want
            - And you can instead optionally ask for however many transitions from however many
                trajs from however many tasks

        - You need to form train and test splits

        - !!!!!!! Run first meta-irl experiments with ground-truth "latents" so you don't have to think
            about encoding the transitions !!!!!!!!
            - So essentially make sure that you can train GAIL with directly using the task params
        
        - Train the full meta-learning method

    - Add possibility of using the pixel wrapper rendering just for evaluation so you can
        see that it is doing well


- Making things work with deepmind control suite
    - (done) wrapper for non pixel version
    - (done) make the wrapper be able to give both a pixel obervation AND the concatenated vector
    - (done) make rlkit work with a dictionary of observations
    - (done) make rl algorithms take pixels and obs or just pixels or just obs
    - (done) make irl algorithms take pixels and obs or just pixels or just obs
    - (done) Debug
    - Add a reward for being in the target zone and change the distance to a difference of distance
        so that it acts as a potential function
    - implement setter function for SimpleReplayBuffer for the policy_uses_pixels variable
    - Test GAIL on reacher with DCS
        - Run reacher and save one eval trajectory per eval cycle
        - Tune hyperparameters for the GAIL reacher and make sure it's working


Thinking about tasks:
    - The problem with sequential tasks and pretty complicated tasks is that GAIL probably won't work or it'll
        be really hard to make GAIL work
    - Need to figure out structured rewards or something for that
    - You also have to design the tasks such that they are not partially observable



- Try training GAIL from images

- (done_ Fix batches for AIRL
- (done) Debug AIRL script
- (done) Keep track of the discriminator classification accuracy too and plot it

- (done) Implement GAIL
    - (done) Do the basic GAIL implementation stuff
    - (done) Figure out batch size, how many trajs, etc.
    - (done) Reduce the number of expert trajs
    - (done) Implement a discriminator with Tanh 2 layers 100 hid size and no dropout
    - (done) I added the WGAN-GP version Add one of the two types of gradient penalty
    - (done) Make the gail run_script
    - (done) Figure out the learning rates and stuff
    - (done) Try training it
    - (done) Try making discriminator stronger to see if it reduces policy variance
    - (done) Make it off-policy by increasing the replay buffer size
- (done) used WGAN-GP, play with using gradient penalty (DRAGAN penalty vs. WGAN-GP penalty)
- (done) ReLU + scale 5 works well: need to also play with reward scale for SAC

- Make AIRL work for pendulum:
    - (done) update the AIRL script
    - (done) compute disc accuracy, reward mean and std
    - (done) add gradient penalty
    - (done) READ THROUGH AIRL AND MAKE SURE THE OBJECTIVE IS CORRECT
    - Play with the batch sizes to figure out what is happening
    - compare gradient penalty on the reward function, exp of the reward, or discriminator output
    - compute the integral of the reward function for pendulum

Other things to try for GAIL:
    - How much does the log(D) - log(1-D) improve things compared to log(D) or -log(1-D)
    - Think about how to regularize policy optimization for SAC, maybe some trust region method or KFAC

- For making AIRL work, first use the expert policy Advantage as the reward function (don't learn the reward),
    and use a lot of expert samples
- Keep track of the integral of the reward function. To approximate it use importance sampling with
    the policy output distribution
- Run some experiments

- Maybe first try to get GAIL working with SAC?
- Right now batches are being sampled reandomly from the buffer, not contiguosly so that they are
    from the same episode. Try contigous.
- Maybe a hybrid might even be better where you take random contigous segments?

- play with capacity of the reward model
- also store the pretanh values so you don't get numerical problems
- play with how many D vs. G
- right now keeping replay buffer the same size as batch size, try making it a multiple
    the batch size so that you are sampling from the past few version of the policy
- GAIL and AIRL all use TRPO which keeps to policy from changing too much per iteration, maybe
    you need a regularizer like that too for SAC
- play with dropout no dropout
- play with batch size, number of updates
- play with hid_dim for the reward model
- Think about regularizing the reward function so it doesn't get too big or too small
- play with reward scale for SAC if necessary


THINGS TO TRY AGAIN IN HARDER TASKS:
- Make it off-policy by increasing the replay buffer size
