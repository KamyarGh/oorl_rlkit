IMPLEMENTATION STUFF:
- implement few shot evaluation script
- implement few shot np_encoder
- fixing a broken ELBO KL term
    - tune it on np_bc
- 

THINGS LEFT TO TRY:
larger disc models
disc having momentum in Adam
expert demos for policy optimization
(done) (made things more stable) Reduce disc learning rate
adam optimizer for the gate







FOR MAKING THE REACH TASK WORK:
    - generate demonstrations for the zero single reach task
    - train it using the best big_gan_hyper_params model


- Might need to turn off target disc


- temp of the SAC algorithm maybe should be annealed and maybe need to try new temps



- In the old version of the architectures must also clip the gradient of the grad pen loss!!!! (probably 10.0 or so)
- Check if the noise in the beginning is actually necessary cause as is I have to run things for at least 100 epochs

- try disc architecture with inductive bias
    - try my initial version
    - try my second version
    - if not worked put exact attention mask on second version
    - if not worked make the color be exactly -1-1-1 and the other exactly 111
    - try with action embedding as well
- try with policy also having a "target" policy








- try with less disc iters
- try more training data (4000)
- try disc with other params changed
- try disc with relus
- make cuda traing + render possible

- discriminator might be able to cheat just by memorizing the specific colors that show up in the
    expert trajectory episodes
- is it necessary that the policy batch for disc training is also sampled in a trajectory-oriented way?

- make things work with last 5 and no KL
- make things work without last 5 and just subsampling, no KL
- make things work with the previous plus KL
- make things work with the previous plus variable number of context trajs

Train few shot fetch:
    - (done) Train lift to center basic models to get a sense of good params
        - (done) Get expert data
        - (done) train the bc
        - (done) train the airl
    - Train the few shot version
        - (done) Fix the few shot env to have small target range
        - (done) Make sure the expert demonstrations are good (visualize them!)
        - (done) Generate the expert data for lifting to center
        - (done) implement a way to get an estimate of an upper bound on how well the model could do

        - visualize the np_bc models
        - try np_bc with reparam tanh multivariate gaussian with proper regularizations
        - first plan your day
        - check out why max-min values for the expert demos were weird
        - if the np_bc model does not train, reduce to like only 5 meta-train tasks, without a meta-test
            and see how well you could overfit OR increase the amount of demonstrations
        
        - start writing
        
        - (done) fix the sampling for obtain eval_samples to be unique
        - (done) fix how you sample random batches, you should sample trajectories then sample batches from the trajs

        - play with different more reasonable architecture sizes for the encoder
            - while these are training implement the stuff you need for np_airl
            - might need to try reparam multivariate tanh for np_bc as well

        - Batching for np_airl is making things weird for grad_pen
        - FIRST THING SET UP NP_AIRL
            - run the 100 dim policy versions on cpu with a lot of setting
            - run reasonable hyperparameters (up to 16 experiments) on GPUs as well
        

        - try much smaller encoder and z dims (maybe 50) to see what happens, even smaller policies (like 2 dims)
        - try more number of tasks per update
        - train the np_bc version to figure out architecture sizes and stuff and make sure the same color radius is good
            - remove the traj_len 5 and instead add subsampling
                - figure out how much subsampling to use
                - also try np_bc with convnets and maybe even LSTM
        - train the np_airl version
            - add the thing that for each task params setting you generate some number of trajectories before you start training
    - Before you run the few shot experiments make sure you can reload the policies from checkpoints and that
        you somehow always save the best checkpoint in addition to all other checkpoints
    - Add the KL term to this
    - Try with variable number of context trajs
    - Try with a variation where you'd train the decoder part for a few iters then backprop to the encoder part
    - np_airl has an advantage that it can see a lot more combinations of context and test colors, maybe also do
        a version which the exact colors that are used in episodes are the ones from the expert demos

Few Shot + Uncertainty Evaluation:
    - Implement evaluation script
    - Implement having variable number of context trajs











- Add evaluation using both train and test task settings
- NEED to fix in np_bc and np_airl sample_random_batch
    - I should not be using sample_random_batch since it samples from all available trajectories
    - I should be sampling the batch from a sampled subset of trajectories
- Implement something that automatically looks at expert replay buffer
    and performs the appropriate scalings
- Make sure few shot fetch env is actually working as well as the expert trajs for it are good
    - something was weird about the range of observation values
- Implement a scaling wrapper

- instead of the min-max thing could maybe try whitening by computing a covariance matrix


- make the environment
    - make sure it satisfies the interface from meta-irl envs
- generate demos
- convert the demos to proper format
- update np_airl and np_bc to also maximize log prob(context)
- run experiments
- make necessary modifications for running with different context sizes
- think about the hierarchical thing
    - gatherer env
    - maybe some blocks on top of one another and need to be moved
    - learning to use an API
    - Kevin Ellis stuff
- implement the few shot evaluation thing
- Also should try something where disc only sees the state and for example we change the dynamics
    and we show that it still works
- add the KL thing

- need to try initial disc iters
- need to try with terminal state




- need to handle how it takes very different amounts of time to complete the tasks for fetch (implement terminal states)

- (done) make extra easy env
- (done) generate noisy demos for extra easy env
- modify the conversion script to also add the discriminator observations
- convert the demos

- how many updates per iter
- how many iters to not train for in the beginning
- see how that gail pick and place/stack thing paper did it
- batch norm
- make demonstrations more temporally extended
- make demonstrations more noisy
- add initial D iters

- (done) reduce the learning rate
- (done) change the demos so they are clipped between -1.0 and 1.0
    - (done) get the demos
    - (done) convert them and add to expert list
- run fetch bc with normalized obs as well
    - run it with normalized
    - also add achieved goal to the observations as well
    - run this version as well
    - look up how they normalize/preprocess themselves in the HER code
    - implement anything else that you might need
- run experiments for AIRL

Getting Fetch Results:
    - (done) Figure out how to make the data generation script work
    - Figure out how to get the HER stuff training on multiple cpus


AIRL for Fetch:
    - Read what the diff between the original pick & place and the new one is
    - (done) Convert demonstrations to the right format
    - add the environment and env script (or see how you can call it from GYM)
    - Run AIRL with policy and Q-func that has similar network size as the one 



Implementing Neural Process Meta-Learner first verions:
    - (done) implement the main class
    - (done) buffers:
        - simple replay buffer
        - env replay buffer
    - (done) add sample trajs function to SimpleReplayBuffer
    - (already done) updating meta env classes so that you can specificy what environment you want to get
    - (done) implement meta irl algorithm
        - (don't need this I think) meta in place policy sampler
        - (don't need this rn I think) implement sampling from the policy for specific task params
    - (done) handle giving task identifiers to meta-irl
    - (done) add torch meta irl algorithm
    - (done) implement a basic trajectory encoder
        - (done) finish implementing r to z map
    - (done) fix the interface between encoder and the MetaAIRL algorithm

    - (done) implement an instance of torch rl algorithm that generates "meta-expert-data"
    - (done) fix the interface between np_bc and meta-expert-sampler
    - (done) implement train_online for meta irl
    - (done) implement policies that condition on z
    - (done) implement the two get expert trajs
    - (done) implement get exploration policy
    - (done) fix the trivial encoder
    - (done) fix _do_training
        - make the encoder and policy training look nicer
    - (done) implement obtain_eval_samples
    - (done) implement evaluate in torch meta irl

    - (done) make a script for populating a meta simple replay buffer with expert data
    - (done) debug it
    - (done) debug the meta simple replay buffer
    - (done) fix meta-irl init parameters
    - (done) use the pretrained simple meta reacher expert to generate meta expert trajs
        - fix up the expert traj generation algorithm
    - (done) debug np_bc
    - (done) implement the training script for np_bc
        - use an MlpPolicy
    
    - (done) debug task_identifier
    - (done) debug trivialencoder

    - (done) run initial experiments for np_bc

    - DONT FORGET TO REMOVE:
        - z is not being sample, taking the means
        - trivial encoder is only taking the last 5 timesteps (also has to be fixed in train_np_bc where you set input_size)

    - implement subsampling for meta-simple-replay-buffer
    - !!!! don't forget to add the KL for the elbo objective !!!!
    - add scheduling for the KL
    - fixing a broken ELBO


    - !!!! don't forget to add the KL for the elbo objective !!!!
    - debug np_airl
    - add scheduling for the KL
    - run np_airl experiments

    - need to clean up task_params, obs_task_params, task_identifier
        - also consider that maybe expert has access to extra information that is not constant
            throughout an episode and hence cannot be folded into obs_task_params

    - ?? do the relabeling trick ??

    - I should add an online flag to meta-irl so that the meta train and meta test sets don't need to be finite


DEBUGGIN NP_AIRL AND MAKING IT WORK WITHOUT KL:
    - read over to code to make sure you didn't fuck up
        - e.g. detaching etc.
        
    - The disc is not able to get perfect accuracy
        - (trying) make discriminator have 3 layers instead of two
        - The whole model is decently big and you don't have batch norm anywhere except the disc
        - Use the LSTM version of things
        - Last resort make models larger



Figure out why things are running so slowly!!!!!!!
    - First check if pixel rendering without saving is the culprit or saving the buffer or loading it

Parallel Data Gathering & Training:
    - MPI stuff

Refactor:
    - REALLY need to refactor this code at some point to make the 1) DMCS and 2) the meta-learning
        stuff not possible to have stupid random bugs inside

Evaluation Scripts:
    - Write a script that takes the save directory and the specific exp_specs variant and evaluates it
        and generates pixels, then saves the pixels to a directory so that you can see if the model is
        actually working or not more easily

Getting Meta-IRL to Work:
    - (done) Implement dmcs for meta-rl algorithms
        - (done already) Need a new type of replay buffer where we keep track of the task-defining parameters
    - Implement the meta-reacher environment
        - (done) Implement a meta-environment for DMCS
    - (done) YOU WOULD MAYBE WANT TO ADD NEW OBJECTS ETC. TO THE PHYSICS ENGINE SO YOU WOULD NEED
        TO DO THIS IN THE ENVIRONMENT NOT THE TASK. HENCE JUST FINISH THE WAY YOU WERE DOING IT NOW!
    - (done) Run meta-reacher experiment
        - make sure that the shaped rewards are correct

    !!!!!!!!!!!!!!!!! IMPLEMENT THE PIPELINE FOR TAKING A TRAINED MODEL AND GETTING EXPERT REPLAY BUFFER !!!!!!!!!!!
    - Implement script for taking a trained model checkpoint and generating expert trajectories for:
        - Non meta IRL
        - Meta IRL

    - Initial Meta-IRL experiments:
        - Train a GAIL for simple meta-reacher but not meta version, i.e. concatenate the task params to the
            input of the policy that is being trained with GAIL
        - Train the meta-learning version where you just encoder the last K timesteps (K small, concat them, pass
            to an MLP)

    - !!!!!! Before you do this you need to write the data loader for meta-expert replay buffer
        so that you know what format to save the expert demonstrations in !!!!!!
    - !!!!!! Write a script for generating train/test splits for expert trajs !!!!!!
        - You need a script that first generates a set of train and test task params
        - Then per train and test param generates two sets of trajectories for train and test

    - Implement meta-irl algorithms
        - And for makings things run a lot faster for expert data generation, first train the expert
            with the priviledge information, then once done training load the expert and THEN generate
            the pixels. This will make things run so much faster
        - Need meta-train and meta-test splits for expert demonstrations
        - Since expert demonstrations do not change during training, instead of using the replay buffer
            implementation, use a pytorch data-loader to make things run a lot faster
        - Implement a new type of expert replay buffer from which you can:
            - Sample K tasks
            - And from each task get however many trajectories you want
            - And you can instead optionally ask for however many transitions from however many
                trajs from however many tasks

        - You need to form train and test splits

        - !!!!!!! Run first meta-irl experiments with ground-truth "latents" so you don't have to think
            about encoding the transitions !!!!!!!!
            - So essentially make sure that you can train GAIL with directly using the task params
        
        - Train the full meta-learning method

    - Add possibility of using the pixel wrapper rendering just for evaluation so you can
        see that it is doing well


- Making things work with deepmind control suite
    - (done) wrapper for non pixel version
    - (done) make the wrapper be able to give both a pixel obervation AND the concatenated vector
    - (done) make rlkit work with a dictionary of observations
    - (done) make rl algorithms take pixels and obs or just pixels or just obs
    - (done) make irl algorithms take pixels and obs or just pixels or just obs
    - (done) Debug
    - Add a reward for being in the target zone and change the distance to a difference of distance
        so that it acts as a potential function
    - implement setter function for SimpleReplayBuffer for the policy_uses_pixels variable
    - Test GAIL on reacher with DCS
        - Run reacher and save one eval trajectory per eval cycle
        - Tune hyperparameters for the GAIL reacher and make sure it's working


Thinking about tasks:
    - The problem with sequential tasks and pretty complicated tasks is that GAIL probably won't work or it'll
        be really hard to make GAIL work
    - Need to figure out structured rewards or something for that
    - You also have to design the tasks such that they are not partially observable



- Try training GAIL from images

- (done_ Fix batches for AIRL
- (done) Debug AIRL script
- (done) Keep track of the discriminator classification accuracy too and plot it

- (done) Implement GAIL
    - (done) Do the basic GAIL implementation stuff
    - (done) Figure out batch size, how many trajs, etc.
    - (done) Reduce the number of expert trajs
    - (done) Implement a discriminator with Tanh 2 layers 100 hid size and no dropout
    - (done) I added the WGAN-GP version Add one of the two types of gradient penalty
    - (done) Make the gail run_script
    - (done) Figure out the learning rates and stuff
    - (done) Try training it
    - (done) Try making discriminator stronger to see if it reduces policy variance
    - (done) Make it off-policy by increasing the replay buffer size
- (done) used WGAN-GP, play with using gradient penalty (DRAGAN penalty vs. WGAN-GP penalty)
- (done) ReLU + scale 5 works well: need to also play with reward scale for SAC

- Make AIRL work for pendulum:
    - (done) update the AIRL script
    - (done) compute disc accuracy, reward mean and std
    - (done) add gradient penalty
    - (done) READ THROUGH AIRL AND MAKE SURE THE OBJECTIVE IS CORRECT
    - Play with the batch sizes to figure out what is happening
    - compare gradient penalty on the reward function, exp of the reward, or discriminator output
    - compute the integral of the reward function for pendulum

Other things to try for GAIL:
    - How much does the log(D) - log(1-D) improve things compared to log(D) or -log(1-D)
    - Think about how to regularize policy optimization for SAC, maybe some trust region method or KFAC

- For making AIRL work, first use the expert policy Advantage as the reward function (don't learn the reward),
    and use a lot of expert samples
- Keep track of the integral of the reward function. To approximate it use importance sampling with
    the policy output distribution
- Run some experiments

- Maybe first try to get GAIL working with SAC?
- Right now batches are being sampled reandomly from the buffer, not contiguosly so that they are
    from the same episode. Try contigous.
- Maybe a hybrid might even be better where you take random contigous segments?

- play with capacity of the reward model
- also store the pretanh values so you don't get numerical problems
- play with how many D vs. G
- right now keeping replay buffer the same size as batch size, try making it a multiple
    the batch size so that you are sampling from the past few version of the policy
- GAIL and AIRL all use TRPO which keeps to policy from changing too much per iteration, maybe
    you need a regularizer like that too for SAC
- play with dropout no dropout
- play with batch size, number of updates
- play with hid_dim for the reward model
- Think about regularizing the reward function so it doesn't get too big or too small
- play with reward scale for SAC if necessary


THINGS TO TRY AGAIN IN HARDER TASKS:
- Make it off-policy by increasing the replay buffer size
