2018-07-26 14:46:18.920618 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #0 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                         1.01116
VF Loss                         0.516999
Policy Loss                     0.521125
Q Predictions Mean             -0.00401295
Q Predictions Std               0.00119198
Q Predictions Max              -0.00121981
Q Predictions Min              -0.00654188
V Predictions Mean              0.00196925
V Predictions Std               0.000346805
V Predictions Max               0.00259075
V Predictions Min               0.00142992
Log Pis Mean                   -0.689503
Log Pis Std                     0.223326
Log Pis Max                    -0.265887
Log Pis Min                    -0.919742
Policy mu Mean                 -0.00129269
Policy mu Std                   7.25351e-05
Policy mu Max                  -0.00116758
Policy mu Min                  -0.00154464
Policy log std Mean             0.000774958
Policy log std Std              8.30711e-05
Policy log std Max              0.000847569
Policy log std Min              0.00040991
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              14.3571
Test Returns Std                1.01469
Test Returns Max               16
Test Returns Min               13
Test Actions Mean               0.0154591
Test Actions Std                0.00015585
Test Actions Max                0.0158507
Test Actions Min                0.0152686
Num Paths                     179
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean        5.58101
Exploration Returns Std         2.93655
Exploration Returns Max        16
Exploration Returns Min         3
Exploration Actions Mean       -0.0383685
Exploration Actions Std         0.600053
Exploration Actions Max         0.991599
Exploration Actions Min        -0.997965
AverageReturn                  14.3571
Number of train steps total   873
Number of env steps total    1000
Number of rollouts total      179
Train Time (s)                 23.193
(Previous) Eval Time (s)        0
Sample Time (s)                 1.45777
Epoch Time (s)                 24.6508
Total Train Time (s)           26.4341
Epoch                           0
---------------------------  --------------
2018-07-26 14:46:20.708692 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #0 | Epoch Duration: 26.467612504959106
2018-07-26 14:46:20.709281 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #0 | Started Training: True
2018-07-26 14:46:47.823647 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #1 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                         0.0697986
VF Loss                         0.00697394
Policy Loss                     0.00246954
Q Predictions Mean              1.60642
Q Predictions Std               0.0573673
Q Predictions Max               1.74058
Q Predictions Min               1.44222
V Predictions Mean              2.28913
V Predictions Std               0.068302
V Predictions Max               2.49339
V Predictions Min               2.13556
Log Pis Mean                   -0.674593
Log Pis Std                     0.0814426
Log Pis Max                    -0.534681
Log Pis Min                    -0.901197
Policy mu Mean                  0.0153367
Policy mu Std                   0.000695172
Policy mu Max                   0.0180488
Policy mu Min                   0.0145401
Policy log std Mean            -0.15553
Policy log std Std              0.0122812
Policy log std Max             -0.13772
Policy log std Min             -0.195138
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              11.4205
Test Returns Std                0.558439
Test Returns Max               13
Test Returns Min               11
Test Actions Mean               0.0278451
Test Actions Std                0.0001279
Test Actions Max                0.0281783
Test Actions Min                0.0276786
Num Paths                     174
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean        5.74138
Exploration Returns Std         3.49331
Exploration Returns Max        19
Exploration Returns Min         3
Exploration Actions Mean       -0.0148531
Exploration Actions Std         0.593778
Exploration Actions Max         0.983116
Exploration Actions Min        -0.990094
AverageReturn                  11.4205
Number of train steps total  1873
Number of env steps total    2000
Number of rollouts total      353
Train Time (s)                 25.967
(Previous) Eval Time (s)        1.79304
Sample Time (s)                 1.11572
Epoch Time (s)                 28.8758
Total Train Time (s)           55.2773
Epoch                           1
---------------------------  --------------
2018-07-26 14:46:49.570929 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #1 | Epoch Duration: 28.861164331436157
2018-07-26 14:46:49.571256 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #1 | Started Training: True
2018-07-26 14:47:16.031056 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #2 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                         0.535487
VF Loss                         0.0161253
Policy Loss                     0.0361099
Q Predictions Mean              2.55317
Q Predictions Std               0.108785
Q Predictions Max               2.83357
Q Predictions Min               2.29912
V Predictions Mean              3.22667
V Predictions Std               0.0714635
V Predictions Max               3.41663
V Predictions Min               3.06516
Log Pis Mean                   -0.704206
Log Pis Std                     0.117821
Log Pis Max                    -0.474337
Log Pis Min                    -1.21759
Policy mu Mean                  0.0276534
Policy mu Std                   0.000375175
Policy mu Max                   0.0286673
Policy mu Min                   0.0268589
Policy log std Mean            -0.130439
Policy log std Std              0.0036207
Policy log std Max             -0.124961
Policy log std Min             -0.141302
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              26.6053
Test Returns Std                1.47861
Test Returns Max               29
Test Returns Min               24
Test Actions Mean              -0.0275226
Test Actions Std                0.025967
Test Actions Max                0.0679303
Test Actions Min               -0.05902
Num Paths                     181
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean        5.50829
Exploration Returns Std         3.19983
Exploration Returns Max        19
Exploration Returns Min         3
Exploration Actions Mean        0.031829
Exploration Actions Std         0.58088
Exploration Actions Max         0.996243
Exploration Actions Min        -0.994412
AverageReturn                  26.6053
Number of train steps total  2873
Number of env steps total    3000
Number of rollouts total      534
Train Time (s)                 25.3399
(Previous) Eval Time (s)        1.75417
Sample Time (s)                 1.09055
Epoch Time (s)                 28.1846
Total Train Time (s)           83.4455
Epoch                           2
---------------------------  --------------
2018-07-26 14:47:17.765000 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #2 | Epoch Duration: 28.19338035583496
2018-07-26 14:47:17.765474 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #2 | Started Training: True
2018-07-26 14:47:44.825162 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #3 | Collecting samples for evaluation
---------------------------  ------------
QF Loss                         0.49123
VF Loss                         0.0925082
Policy Loss                     0.0215357
Q Predictions Mean              3.61104
Q Predictions Std               0.800097
Q Predictions Max               4.3861
Q Predictions Min               0.820522
V Predictions Mean              4.26506
V Predictions Std               0.616613
V Predictions Max               5.0492
V Predictions Min               2.46114
Log Pis Mean                   -0.655377
Log Pis Std                     0.251575
Log Pis Max                     0.0504359
Log Pis Min                    -1.51446
Policy mu Mean                  0.0403663
Policy mu Std                   0.194484
Policy mu Max                   0.393148
Policy mu Min                  -0.309066
Policy log std Mean            -0.174122
Policy log std Std              0.0167816
Policy log std Max             -0.139865
Policy log std Min             -0.22313
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              25.2
Test Returns Std                0.927362
Test Returns Max               27
Test Returns Min               23
Test Actions Mean              -0.0349289
Test Actions Std                0.0539303
Test Actions Max                0.102907
Test Actions Min               -0.170464
Num Paths                     127
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean        7.91339
Exploration Returns Std         6.28462
Exploration Returns Max        37
Exploration Returns Min         3
Exploration Actions Mean        0.0199569
Exploration Actions Std         0.597877
Exploration Actions Max         0.992801
Exploration Actions Min        -0.986458
AverageReturn                  25.2
Number of train steps total  3873
Number of env steps total    4000
Number of rollouts total      661
Train Time (s)                 26.0773
(Previous) Eval Time (s)        1.73854
Sample Time (s)                 0.952514
Epoch Time (s)                 28.7683
Total Train Time (s)          112.142
Epoch                           3
---------------------------  ------------
2018-07-26 14:47:46.486245 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #3 | Epoch Duration: 28.720349311828613
2018-07-26 14:47:46.486588 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #3 | Started Training: True
2018-07-26 14:48:15.593748 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #4 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                         0.269684
VF Loss                         0.062226
Policy Loss                     0.00813294
Q Predictions Mean              4.70152
Q Predictions Std               1.36578
Q Predictions Max               5.81191
Q Predictions Min               0.258282
V Predictions Mean              5.40812
V Predictions Std               1.15806
V Predictions Max               6.61549
V Predictions Min               1.41922
Log Pis Mean                   -0.612152
Log Pis Std                     0.39838
Log Pis Max                     0.528912
Log Pis Min                    -2.10526
Policy mu Mean                  0.0136074
Policy mu Std                   0.400523
Policy mu Max                   0.804446
Policy mu Min                  -0.884444
Policy log std Mean            -0.237151
Policy log std Std              0.0652656
Policy log std Max             -0.156749
Policy log std Min             -0.411097
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              39.1538
Test Returns Std                1.91537
Test Returns Max               43
Test Returns Min               36
Test Actions Mean              -0.0263457
Test Actions Std                0.0249006
Test Actions Max                0.109116
Test Actions Min               -0.0746726
Num Paths                      71
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean       13.5211
Exploration Returns Std        11.0961
Exploration Returns Max        65
Exploration Returns Min         3
Exploration Actions Mean        0.00883394
Exploration Actions Std         0.598881
Exploration Actions Max         0.9959
Exploration Actions Min        -0.986979
AverageReturn                  39.1538
Number of train steps total  4873
Number of env steps total    5000
Number of rollouts total      732
Train Time (s)                 27.9908
(Previous) Eval Time (s)        1.66597
Sample Time (s)                 1.08644
Epoch Time (s)                 30.7432
Total Train Time (s)          142.936
Epoch                           4
---------------------------  -------------
2018-07-26 14:48:17.308036 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #4 | Epoch Duration: 30.821145057678223
2018-07-26 14:48:17.308504 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #4 | Started Training: True
2018-07-26 14:48:45.509287 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #5 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                         0.509041
VF Loss                         0.12382
Policy Loss                    -0.061123
Q Predictions Mean              5.28726
Q Predictions Std               2.01239
Q Predictions Max               6.9267
Q Predictions Min               0.147627
V Predictions Mean              6.15066
V Predictions Std               1.86254
V Predictions Max               7.55939
V Predictions Min               1.30387
Log Pis Mean                   -0.470078
Log Pis Std                     0.75616
Log Pis Max                     1.03505
Log Pis Min                    -4.21301
Policy mu Mean                  0.0294753
Policy mu Std                   0.598536
Policy mu Max                   1.2363
Policy mu Min                  -1.26684
Policy log std Mean            -0.350545
Policy log std Std              0.127436
Policy log std Max              0.0507144
Policy log std Min             -0.648954
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              61.7059
Test Returns Std                2.60688
Test Returns Max               67
Test Returns Min               58
Test Actions Mean              -0.024704
Test Actions Std                0.0224948
Test Actions Max                0.0897387
Test Actions Min               -0.0830984
Num Paths                      35
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean       29.6857
Exploration Returns Std        20.4014
Exploration Returns Max       112
Exploration Returns Min         3
Exploration Actions Mean       -0.00541779
Exploration Actions Std         0.571422
Exploration Actions Max         0.982893
Exploration Actions Min        -0.989495
AverageReturn                  61.7059
Number of train steps total  5873
Number of env steps total    6000
Number of rollouts total      767
Train Time (s)                 27.2714
(Previous) Eval Time (s)        1.71989
Sample Time (s)                 0.900006
Epoch Time (s)                 29.8913
Total Train Time (s)          172.762
Epoch                           5
---------------------------  -------------
2018-07-26 14:48:47.157086 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #5 | Epoch Duration: 29.848201513290405
2018-07-26 14:48:47.157421 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #5 | Started Training: True
2018-07-26 14:49:15.146115 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #6 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                         0.51199
VF Loss                         0.120312
Policy Loss                    -0.0621419
Q Predictions Mean              6.47114
Q Predictions Std               2.38455
Q Predictions Max               8.35171
Q Predictions Min               0.519204
V Predictions Mean              7.23678
V Predictions Std               2.04552
V Predictions Max               8.54557
V Predictions Min               1.05065
Log Pis Mean                   -0.332679
Log Pis Std                     0.824245
Log Pis Max                     1.73801
Log Pis Min                    -3.77097
Policy mu Mean                 -0.0471091
Policy mu Std                   0.649437
Policy mu Max                   1.35983
Policy mu Min                  -1.4317
Policy log std Mean            -0.458071
Policy log std Std              0.154161
Policy log std Max              0.111154
Policy log std Min             -0.741218
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              78.3846
Test Returns Std                2.84303
Test Returns Max               83
Test Returns Min               74
Test Actions Mean               0.0225435
Test Actions Std                0.0467632
Test Actions Max                0.798527
Test Actions Min               -0.0621188
Num Paths                      20
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean       48
Exploration Returns Std        26.9833
Exploration Returns Max       115
Exploration Returns Min         5
Exploration Actions Mean       -0.00652979
Exploration Actions Std         0.559479
Exploration Actions Max         0.984561
Exploration Actions Min        -0.98121
AverageReturn                  78.3846
Number of train steps total  6873
Number of env steps total    7000
Number of rollouts total      787
Train Time (s)                 27.0522
(Previous) Eval Time (s)        1.65265
Sample Time (s)                 0.907352
Epoch Time (s)                 29.6122
Total Train Time (s)          202.409
Epoch                           6
---------------------------  -------------
2018-07-26 14:49:16.828694 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #6 | Epoch Duration: 29.670965671539307
2018-07-26 14:49:16.829094 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #6 | Started Training: True
2018-07-26 14:49:45.249424 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #7 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                         0.33699
VF Loss                         0.168065
Policy Loss                    -0.0181009
Q Predictions Mean              7.52885
Q Predictions Std               2.7355
Q Predictions Max               9.54217
Q Predictions Min               0.665899
V Predictions Mean              8.50845
V Predictions Std               1.81693
V Predictions Max               9.63726
V Predictions Min               1.20751
Log Pis Mean                   -0.0999115
Log Pis Std                     0.831625
Log Pis Max                     1.7572
Log Pis Min                    -3.03607
Policy mu Mean                 -0.171255
Policy mu Std                   0.723988
Policy mu Max                   1.36673
Policy mu Min                  -1.54904
Policy log std Mean            -0.584132
Policy log std Std              0.129213
Policy log std Max              0.00633031
Policy log std Min             -0.790708
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean              65.75
Test Returns Std                1.63936
Test Returns Max               68
Test Returns Min               62
Test Actions Mean              -0.032677
Test Actions Std                0.085317
Test Actions Max                0.0958949
Test Actions Min               -0.921725
Num Paths                      14
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean       63.2143
Exploration Returns Std        28.4409
Exploration Returns Max       117
Exploration Returns Min         4
Exploration Actions Mean       -0.0139224
Exploration Actions Std         0.538543
Exploration Actions Max         0.98643
Exploration Actions Min        -0.96176
AverageReturn                  65.75
Number of train steps total  7873
Number of env steps total    8000
Number of rollouts total      801
Train Time (s)                 27.5205
(Previous) Eval Time (s)        1.68739
Sample Time (s)                 0.871267
Epoch Time (s)                 30.0792
Total Train Time (s)          232.448
Epoch                           7
---------------------------  -------------
2018-07-26 14:49:46.892029 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #7 | Epoch Duration: 30.06260085105896
2018-07-26 14:49:46.892343 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #7 | Started Training: True
2018-07-26 14:50:15.975424 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #8 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                         0.396436
VF Loss                         0.257355
Policy Loss                    -0.101347
Q Predictions Mean              8.67397
Q Predictions Std               2.91963
Q Predictions Max              10.6374
Q Predictions Min               0.368173
V Predictions Mean              9.84798
V Predictions Std               2.12569
V Predictions Max              11.0554
V Predictions Min               1.23464
Log Pis Mean                   -0.23526
Log Pis Std                     0.783404
Log Pis Max                     1.73896
Log Pis Min                    -4.45876
Policy mu Mean                  0.103826
Policy mu Std                   0.66675
Policy mu Max                   1.62564
Policy mu Min                  -1.43541
Policy log std Mean            -0.574873
Policy log std Std              0.132809
Policy log std Max             -0.00780455
Policy log std Min             -0.791331
Test Rewards Mean               1
Test Rewards Std                0
Test Rewards Max                1
Test Rewards Min                1
Test Returns Mean             107.2
Test Returns Std                6.28967
Test Returns Max              119
Test Returns Min               97
Test Actions Mean              -0.023896
Test Actions Std                0.10221
Test Actions Max                0.0346779
Test Actions Min               -0.948637
Num Paths                      13
Exploration Rewards Mean        1
Exploration Rewards Std         0
Exploration Rewards Max         1
Exploration Rewards Min         1
Exploration Returns Mean       87.3077
Exploration Returns Std        44.3403
Exploration Returns Max       216
Exploration Returns Min        52
Exploration Actions Mean       -0.00858668
Exploration Actions Std         0.554146
Exploration Actions Max         0.979076
Exploration Actions Min        -0.99111
AverageReturn                 107.2
Number of train steps total  8873
Number of env steps total    9000
Number of rollouts total      814
Train Time (s)                 28.1452
(Previous) Eval Time (s)        1.64702
Sample Time (s)                 0.909249
Epoch Time (s)                 30.7015
Total Train Time (s)          263.152
Epoch                           8
---------------------------  -------------
2018-07-26 14:50:17.621483 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #8 | Epoch Duration: 30.728869438171387
2018-07-26 14:50:17.621801 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #8 | Started Training: True
2018-07-26 14:50:46.790282 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #9 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.581751
VF Loss                          0.171405
Policy Loss                      0.0617022
Q Predictions Mean               9.67834
Q Predictions Std                3.27046
Q Predictions Max               11.7792
Q Predictions Min                0.356187
V Predictions Mean              10.9265
V Predictions Std                2.35388
V Predictions Max               12.1888
V Predictions Min                1.01566
Log Pis Mean                    -0.0920825
Log Pis Std                      0.78907
Log Pis Max                      2.34907
Log Pis Min                     -2.80648
Policy mu Mean                  -0.0252852
Policy mu Std                    0.753266
Policy mu Max                    1.68617
Policy mu Min                   -1.91686
Policy log std Mean             -0.595867
Policy log std Std               0.135596
Policy log std Max               0.132578
Policy log std Min              -0.794167
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean               86.4167
Test Returns Std                 1.9773
Test Returns Max                89
Test Returns Min                82
Test Actions Mean               -0.033235
Test Actions Std                 0.127989
Test Actions Max                 0.0814887
Test Actions Min                -0.900385
Num Paths                       12
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean        79.3333
Exploration Returns Std         58.101
Exploration Returns Max        254
Exploration Returns Min         27
Exploration Actions Mean        -0.00455198
Exploration Actions Std          0.562011
Exploration Actions Max          0.982674
Exploration Actions Min         -0.979456
AverageReturn                   86.4167
Number of train steps total   9873
Number of env steps total    10000
Number of rollouts total       826
Train Time (s)                  28.2554
(Previous) Eval Time (s)         1.65008
Sample Time (s)                  0.884526
Epoch Time (s)                  30.79
Total Train Time (s)           293.941
Epoch                            9
---------------------------  --------------
2018-07-26 14:50:48.434341 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #9 | Epoch Duration: 30.81227135658264
2018-07-26 14:50:48.434649 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #9 | Started Training: True
2018-07-26 14:51:17.199224 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #10 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.176052
VF Loss                          0.552543
Policy Loss                     -0.330438
Q Predictions Mean              10.8133
Q Predictions Std                3.75512
Q Predictions Max               13.1158
Q Predictions Min                0.423017
V Predictions Mean              11.7523
V Predictions Std                2.8973
V Predictions Max               13.2163
V Predictions Min                1.08855
Log Pis Mean                    -0.345604
Log Pis Std                      0.827261
Log Pis Max                      1.67489
Log Pis Min                     -3.78472
Policy mu Mean                  -0.0880264
Policy mu Std                    0.663465
Policy mu Max                    1.63347
Policy mu Min                   -1.64507
Policy log std Mean             -0.550825
Policy log std Std               0.138482
Policy log std Max              -0.00145174
Policy log std Min              -0.791834
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean               89
Test Returns Std                 1.35401
Test Returns Max                91
Test Returns Min                87
Test Actions Mean               -0.035316
Test Actions Std                 0.13603
Test Actions Max                 0.101801
Test Actions Min                -0.881695
Num Paths                        8
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       121.375
Exploration Returns Std         59.8747
Exploration Returns Max        263
Exploration Returns Min         70
Exploration Actions Mean         0.00485953
Exploration Actions Std          0.547072
Exploration Actions Max          0.987347
Exploration Actions Min         -0.987575
AverageReturn                   89
Number of train steps total  10873
Number of env steps total    11000
Number of rollouts total       834
Train Time (s)                  27.8499
(Previous) Eval Time (s)         1.649
Sample Time (s)                  0.886499
Epoch Time (s)                  30.3854
Total Train Time (s)           324.349
Epoch                           10
---------------------------  --------------
2018-07-26 14:51:18.866404 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #10 | Epoch Duration: 30.431450366973877
2018-07-26 14:51:18.866755 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #10 | Started Training: True
2018-07-26 14:51:48.164487 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #11 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          0.255238
VF Loss                          0.238532
Policy Loss                      0.0284999
Q Predictions Mean              12.2315
Q Predictions Std                3.62748
Q Predictions Max               14.2548
Q Predictions Min                0.769028
V Predictions Mean              13.1018
V Predictions Std                2.17816
V Predictions Max               14.5015
V Predictions Min                1.31137
Log Pis Mean                    -0.0987034
Log Pis Std                      0.834539
Log Pis Max                      2.38097
Log Pis Min                     -2.94397
Policy mu Mean                   0.024564
Policy mu Std                    0.781385
Policy mu Max                    1.87155
Policy mu Min                   -1.83292
Policy log std Mean             -0.663235
Policy log std Std               0.114014
Policy log std Max              -0.358638
Policy log std Min              -0.840454
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              547.5
Test Returns Std                 9.5
Test Returns Max               557
Test Returns Min               538
Test Actions Mean               -0.00628865
Test Actions Std                 0.065139
Test Actions Max                 0.0797343
Test Actions Min                -0.905863
Num Paths                        5
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       167.4
Exploration Returns Std         76.5835
Exploration Returns Max        256
Exploration Returns Min         66
Exploration Actions Mean         0.000726465
Exploration Actions Std          0.550045
Exploration Actions Max          0.979632
Exploration Actions Min         -0.987939
AverageReturn                  547.5
Number of train steps total  11873
Number of env steps total    12000
Number of rollouts total       839
Train Time (s)                  28.3633
(Previous) Eval Time (s)         1.67157
Sample Time (s)                  0.904658
Epoch Time (s)                  30.9395
Total Train Time (s)           355.273
Epoch                           11
---------------------------  ---------------
2018-07-26 14:51:49.816467 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #11 | Epoch Duration: 30.94937229156494
2018-07-26 14:51:49.816853 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #11 | Started Training: True
2018-07-26 14:52:20.059700 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #12 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.242138
VF Loss                          0.438263
Policy Loss                     -0.130134
Q Predictions Mean              12.6879
Q Predictions Std                4.33192
Q Predictions Max               15.3171
Q Predictions Min                0.49252
V Predictions Mean              14.3362
V Predictions Std                2.5001
V Predictions Max               15.7213
V Predictions Min                1.66514
Log Pis Mean                    -0.184239
Log Pis Std                      0.842091
Log Pis Max                      2.04437
Log Pis Min                     -3.94564
Policy mu Mean                  -0.0791569
Policy mu Std                    0.730508
Policy mu Max                    1.85046
Policy mu Min                   -1.7653
Policy log std Mean             -0.54888
Policy log std Std               0.136035
Policy log std Max               0.017807
Policy log std Min              -0.759323
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              391.333
Test Returns Std                 2.62467
Test Returns Max               395
Test Returns Min               389
Test Actions Mean               -0.0159207
Test Actions Std                 0.0970904
Test Actions Max                 0.0708149
Test Actions Min                -0.863921
Num Paths                        7
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       179.429
Exploration Returns Std        132.238
Exploration Returns Max        446
Exploration Returns Min         43
Exploration Actions Mean         0.00159591
Exploration Actions Std          0.559793
Exploration Actions Max          0.977875
Exploration Actions Min         -0.991661
AverageReturn                  391.333
Number of train steps total  12873
Number of env steps total    13000
Number of rollouts total       846
Train Time (s)                  29.3108
(Previous) Eval Time (s)         1.65684
Sample Time (s)                  0.903148
Epoch Time (s)                  31.8708
Total Train Time (s)           387.194
Epoch                           12
---------------------------  --------------
2018-07-26 14:52:21.760669 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #12 | Epoch Duration: 31.94350576400757
2018-07-26 14:52:21.761054 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #12 | Started Training: True
2018-07-26 14:52:51.028356 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #13 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                          0.0872792
VF Loss                          0.14034
Policy Loss                     -0.0344365
Q Predictions Mean              14.5126
Q Predictions Std                4.10188
Q Predictions Max               16.5835
Q Predictions Min                0.701878
V Predictions Mean              15.4957
V Predictions Std                3.01189
V Predictions Max               16.9264
V Predictions Min                0.23726
Log Pis Mean                    -0.340672
Log Pis Std                      0.773207
Log Pis Max                      1.96138
Log Pis Min                     -3.03355
Policy mu Mean                   0.063116
Policy mu Std                    0.580445
Policy mu Max                    1.94861
Policy mu Min                   -1.31889
Policy log std Mean             -0.642695
Policy log std Std               0.115826
Policy log std Max              -0.149723
Policy log std Min              -0.870451
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              105.2
Test Returns Std                 1.72047
Test Returns Max               107
Test Returns Min               101
Test Actions Mean               -0.0432435
Test Actions Std                 0.162247
Test Actions Max                 0.108992
Test Actions Min                -0.902952
Num Paths                        7
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       137.714
Exploration Returns Std         52.0596
Exploration Returns Max        209
Exploration Returns Min         55
Exploration Actions Mean        -0.0109421
Exploration Actions Std          0.563468
Exploration Actions Max          0.988455
Exploration Actions Min         -0.979794
AverageReturn                  105.2
Number of train steps total  13873
Number of env steps total    14000
Number of rollouts total       853
Train Time (s)                  28.3786
(Previous) Eval Time (s)         1.70685
Sample Time (s)                  0.860302
Epoch Time (s)                  30.9457
Total Train Time (s)           418.116
Epoch                           13
---------------------------  -------------
2018-07-26 14:52:52.706739 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #13 | Epoch Duration: 30.945358753204346
2018-07-26 14:52:52.707081 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #13 | Started Training: True
2018-07-26 14:53:21.772889 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #14 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.468662
VF Loss                          0.157257
Policy Loss                      0.141679
Q Predictions Mean              15.3696
Q Predictions Std                4.2372
Q Predictions Max               18.1818
Q Predictions Min               -0.010653
V Predictions Mean              16.3447
V Predictions Std                3.12115
V Predictions Max               18.2006
V Predictions Min                0.0807751
Log Pis Mean                    -0.213923
Log Pis Std                      0.721449
Log Pis Max                      2.10365
Log Pis Min                     -2.2264
Policy mu Mean                   0.0560045
Policy mu Std                    0.630392
Policy mu Max                    1.57928
Policy mu Min                   -1.43987
Policy log std Mean             -0.637748
Policy log std Std               0.152835
Policy log std Max              -0.0636502
Policy log std Min              -0.891308
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean               87.5
Test Returns Std                 1.04083
Test Returns Max                89
Test Returns Min                86
Test Actions Mean               -0.0459308
Test Actions Std                 0.167949
Test Actions Max                 0.13348
Test Actions Min                -0.906487
Num Paths                        5
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       195
Exploration Returns Std        169.616
Exploration Returns Max        477
Exploration Returns Min         59
Exploration Actions Mean        -0.00297898
Exploration Actions Std          0.562511
Exploration Actions Max          0.983234
Exploration Actions Min         -0.989623
AverageReturn                   87.5
Number of train steps total  14873
Number of env steps total    15000
Number of rollouts total       858
Train Time (s)                  28.1925
(Previous) Eval Time (s)         1.68299
Sample Time (s)                  0.844285
Epoch Time (s)                  30.7198
Total Train Time (s)           448.808
Epoch                           14
---------------------------  --------------
2018-07-26 14:53:23.423135 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #14 | Epoch Duration: 30.715749263763428
2018-07-26 14:53:23.423520 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #14 | Started Training: True
2018-07-26 14:53:52.514220 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #15 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.723324
VF Loss                          0.233068
Policy Loss                      0.0570979
Q Predictions Mean              16.471
Q Predictions Std                3.88071
Q Predictions Max               18.3984
Q Predictions Min               -0.0521669
V Predictions Mean              17.6525
V Predictions Std                2.70974
V Predictions Max               18.9421
V Predictions Min                1.00987
Log Pis Mean                    -0.132396
Log Pis Std                      0.697179
Log Pis Max                      2.65608
Log Pis Min                     -2.3607
Policy mu Mean                  -0.0111646
Policy mu Std                    0.659703
Policy mu Max                    1.96287
Policy mu Min                   -1.87745
Policy log std Mean             -0.64192
Policy log std Std               0.144156
Policy log std Max              -0.054919
Policy log std Min              -0.920965
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              286.5
Test Returns Std                 2.17945
Test Returns Max               290
Test Returns Min               284
Test Actions Mean               -0.0177015
Test Actions Std                 0.108188
Test Actions Max                 0.0641728
Test Actions Min                -0.917201
Num Paths                        4
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       204.5
Exploration Returns Std         96.8517
Exploration Returns Max        362
Exploration Returns Min        119
Exploration Actions Mean         0.00771726
Exploration Actions Std          0.540132
Exploration Actions Max          0.990344
Exploration Actions Min         -0.97839
AverageReturn                  286.5
Number of train steps total  15873
Number of env steps total    16000
Number of rollouts total       862
Train Time (s)                  28.1875
(Previous) Eval Time (s)         1.65549
Sample Time (s)                  0.873264
Epoch Time (s)                  30.7162
Total Train Time (s)           479.574
Epoch                           15
---------------------------  --------------
2018-07-26 14:53:54.213388 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #15 | Epoch Duration: 30.78951382637024
2018-07-26 14:53:54.213716 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #15 | Started Training: True
2018-07-26 14:54:23.020188 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #16 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.691957
VF Loss                          0.121933
Policy Loss                      0.0958341
Q Predictions Mean              17.7537
Q Predictions Std                4.19357
Q Predictions Max               20.2825
Q Predictions Min                0.739052
V Predictions Mean              18.0944
V Predictions Std                3.80718
V Predictions Max               20.2778
V Predictions Min                0.651343
Log Pis Mean                    -0.152404
Log Pis Std                      0.775256
Log Pis Max                      2.32952
Log Pis Min                     -2.41611
Policy mu Mean                  -0.0165564
Policy mu Std                    0.655462
Policy mu Max                    1.60234
Policy mu Min                   -1.97709
Policy log std Mean             -0.649056
Policy log std Std               0.143603
Policy log std Max              -0.0484609
Policy log std Min              -0.924208
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              288.25
Test Returns Std                 0.829156
Test Returns Max               289
Test Returns Min               287
Test Actions Mean               -0.0121721
Test Actions Std                 0.0828435
Test Actions Max                 0.103598
Test Actions Min                -0.911508
Num Paths                        6
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       155.5
Exploration Returns Std         81.0591
Exploration Returns Max        273
Exploration Returns Min         29
Exploration Actions Mean         0.00823133
Exploration Actions Std          0.549206
Exploration Actions Max          0.972367
Exploration Actions Min         -0.976146
AverageReturn                  288.25
Number of train steps total  16873
Number of env steps total    17000
Number of rollouts total       868
Train Time (s)                  27.9167
(Previous) Eval Time (s)         1.70524
Sample Time (s)                  0.851858
Epoch Time (s)                  30.4738
Total Train Time (s)           510.091
Epoch                           16
---------------------------  --------------
2018-07-26 14:54:24.754572 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #16 | Epoch Duration: 30.540559768676758
2018-07-26 14:54:24.754873 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #16 | Started Training: True
2018-07-26 14:54:54.035997 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #17 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                          0.871354
VF Loss                          0.365439
Policy Loss                      0.115777
Q Predictions Mean              18.6868
Q Predictions Std                4.40534
Q Predictions Max               21.2466
Q Predictions Min                0.795327
V Predictions Mean              19.8809
V Predictions Std                2.59105
V Predictions Max               21.2659
V Predictions Min                1.34717
Log Pis Mean                    -0.187182
Log Pis Std                      0.858029
Log Pis Max                      2.85791
Log Pis Min                     -2.72432
Policy mu Mean                  -0.0760974
Policy mu Std                    0.728503
Policy mu Max                    1.79001
Policy mu Min                   -2.02054
Policy log std Mean             -0.675935
Policy log std Std               0.125677
Policy log std Max              -0.344605
Policy log std Min              -0.91033
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              236.8
Test Returns Std                 3.76298
Test Returns Max               242
Test Returns Min               232
Test Actions Mean                0.0167759
Test Actions Std                 0.10175
Test Actions Max                 0.95763
Test Actions Min                -0.0387729
Num Paths                        6
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       212
Exploration Returns Std         99.6895
Exploration Returns Max        339
Exploration Returns Min         51
Exploration Actions Mean        -0.0254054
Exploration Actions Std          0.553669
Exploration Actions Max          0.987843
Exploration Actions Min         -0.989273
AverageReturn                  236.8
Number of train steps total  17873
Number of env steps total    18000
Number of rollouts total       874
Train Time (s)                  28.3401
(Previous) Eval Time (s)         1.74827
Sample Time (s)                  0.91163
Epoch Time (s)                  31
Total Train Time (s)           541.039
Epoch                           17
---------------------------  -------------
2018-07-26 14:54:55.728170 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #17 | Epoch Duration: 30.973033666610718
2018-07-26 14:54:55.728528 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #17 | Started Training: True
2018-07-26 14:55:25.342862 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #18 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                          0.634013
VF Loss                          0.180999
Policy Loss                     -0.0121706
Q Predictions Mean              20.0185
Q Predictions Std                4.6152
Q Predictions Max               22.2957
Q Predictions Min                0.447246
V Predictions Mean              20.79
V Predictions Std                3.74235
V Predictions Max               23.0525
V Predictions Min                0.417248
Log Pis Mean                    -0.231704
Log Pis Std                      0.679468
Log Pis Max                      2.4711
Log Pis Min                     -1.944
Policy mu Mean                  -0.204831
Policy mu Std                    0.574781
Policy mu Max                    1.33896
Policy mu Min                   -2.0331
Policy log std Mean             -0.637301
Policy log std Std               0.149055
Policy log std Max              -0.0954252
Policy log std Min              -0.916021
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              175.167
Test Returns Std                 4.21966
Test Returns Max               182
Test Returns Min               171
Test Actions Mean                0.023429
Test Actions Std                 0.138471
Test Actions Max                 0.962053
Test Actions Min                -0.154237
Num Paths                        6
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       138.333
Exploration Returns Std         74.2286
Exploration Returns Max        242
Exploration Returns Min         49
Exploration Actions Mean         0.00455583
Exploration Actions Std          0.547741
Exploration Actions Max          0.987429
Exploration Actions Min         -0.989632
AverageReturn                  175.167
Number of train steps total  18873
Number of env steps total    19000
Number of rollouts total       880
Train Time (s)                  28.6798
(Previous) Eval Time (s)         1.69754
Sample Time (s)                  0.905294
Epoch Time (s)                  31.2827
Total Train Time (s)           572.299
Epoch                           18
---------------------------  --------------
2018-07-26 14:55:27.014376 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #18 | Epoch Duration: 31.285520792007446
2018-07-26 14:55:27.014854 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #18 | Started Training: True
2018-07-26 14:55:56.277576 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #19 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          0.816885
VF Loss                          0.232583
Policy Loss                     -0.0618024
Q Predictions Mean              21.1801
Q Predictions Std                2.95208
Q Predictions Max               23.3546
Q Predictions Min                1.98501
V Predictions Mean              22.2468
V Predictions Std                1.75719
V Predictions Max               24.105
V Predictions Min                8.28497
Log Pis Mean                    -0.272945
Log Pis Std                      0.678969
Log Pis Max                      2.06903
Log Pis Min                     -2.78734
Policy mu Mean                  -0.000739755
Policy mu Std                    0.573037
Policy mu Max                    1.63009
Policy mu Min                   -1.70853
Policy log std Mean             -0.654163
Policy log std Std               0.122893
Policy log std Max              -0.319419
Policy log std Min              -0.95449
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              232.2
Test Returns Std                 0.748331
Test Returns Max               233
Test Returns Min               231
Test Actions Mean               -0.0139943
Test Actions Std                 0.0799663
Test Actions Max                 0.159138
Test Actions Min                -0.874936
Num Paths                        4
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       283.75
Exploration Returns Std        198.835
Exploration Returns Max        513
Exploration Returns Min         44
Exploration Actions Mean         0.00270786
Exploration Actions Std          0.551355
Exploration Actions Max          0.986847
Exploration Actions Min         -0.977076
AverageReturn                  232.2
Number of train steps total  19873
Number of env steps total    20000
Number of rollouts total       884
Train Time (s)                  28.37
(Previous) Eval Time (s)         1.67721
Sample Time (s)                  0.863879
Epoch Time (s)                  30.9111
Total Train Time (s)           603.206
Epoch                           19
---------------------------  ---------------
2018-07-26 14:55:57.943586 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #19 | Epoch Duration: 30.92834782600403
2018-07-26 14:55:57.943867 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #19 | Started Training: True
2018-07-26 14:56:28.033969 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #20 | Collecting samples for evaluation
---------------------------  -------------
QF Loss                          1.14993
VF Loss                          0.366707
Policy Loss                      0.0857351
Q Predictions Mean              21.2255
Q Predictions Std                5.72703
Q Predictions Max               24.0804
Q Predictions Min                0.290811
V Predictions Mean              22.5711
V Predictions Std                4.19031
V Predictions Max               24.8013
V Predictions Min                0.0625136
Log Pis Mean                    -0.130656
Log Pis Std                      0.862808
Log Pis Max                      3.24583
Log Pis Min                     -3.23902
Policy mu Mean                   0.121765
Policy mu Std                    0.698239
Policy mu Max                    2.17253
Policy mu Min                   -1.37681
Policy log std Mean             -0.72561
Policy log std Std               0.134451
Policy log std Max              -0.337151
Policy log std Min              -1.04768
Test Rewards Mean                1
Test Rewards Std                 0
Test Rewards Max                 1
Test Rewards Min                 1
Test Returns Mean              372.333
Test Returns Std                 3.29983
Test Returns Max               376
Test Returns Min               368
Test Actions Mean               -0.013448
Test Actions Std                 0.096905
Test Actions Max                 0.0432692
Test Actions Min                -0.951679
Num Paths                        7
Exploration Rewards Mean         1
Exploration Rewards Std          0
Exploration Rewards Max          1
Exploration Rewards Min          1
Exploration Returns Mean       134.429
Exploration Returns Std         38.6665
Exploration Returns Max        189
Exploration Returns Min         78
Exploration Actions Mean        -0.0186614
Exploration Actions Std          0.542388
Exploration Actions Max          0.994679
Exploration Actions Min         -0.977161
AverageReturn                  372.333
Number of train steps total  20873
Number of env steps total    21000
Number of rollouts total       891
Train Time (s)                  29.1586
(Previous) Eval Time (s)         1.67111
Sample Time (s)                  0.901247
Epoch Time (s)                  31.731
Total Train Time (s)           635.115
Epoch                           20
---------------------------  -------------
2018-07-26 14:56:29.876958 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #20 | Epoch Duration: 31.93280291557312
2018-07-26 14:56:29.877300 EDT | [name-of-experiment_2018_07_26_14_45_52_0000--s-0] Iteration #20 | Started Training: True
