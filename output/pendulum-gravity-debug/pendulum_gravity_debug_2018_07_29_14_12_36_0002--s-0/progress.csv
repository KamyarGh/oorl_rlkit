Sample Time (s),Test Actions Max,Exploration Actions Std,Exploration Actions Min,QF Loss,Test Actions Mean,Test Rewards Std,Log Pis Std,Exploration Rewards Min,Policy mu Max,Exploration Rewards Std,Q Predictions Mean,V Predictions Max,Number of train steps total,Q Predictions Min,Test Rewards Mean,Test Rewards Min,Test Rewards Max,Train Time (s),Exploration Actions Mean,Total Train Time (s),Exploration Returns Mean,AverageReturn,Test Actions Std,Exploration Actions Max,Exploration Returns Std,Policy mu Mean,Number of rollouts total,Policy log std Max,Q Predictions Std,Policy Loss,Exploration Rewards Mean,Exploration Returns Max,Policy log std Min,Test Returns Min,Log Pis Max,Test Actions Min,Policy log std Mean,Q Predictions Max,V Predictions Min,Num Paths,Number of env steps total,Policy mu Std,Test Returns Std,Epoch,Log Pis Min,Policy log std Std,Exploration Returns Min,Test Returns Mean,V Predictions Mean,VF Loss,Exploration Rewards Max,V Predictions Std,Log Pis Mean,Policy mu Min,Epoch Time (s),Test Returns Max,(Previous) Eval Time (s)
0.5288433451205492,0.102596,0.581914,-0.994744,0.987483,-0.0175714,0.0,0.226918,1.0,0.00178989,0.0,0.00301446,-0.00313447,3492,0.00153186,1.0,1.0,1.0,29.307238473556936,-0.00430212,30.379102523438632,8.68695652174,45.5,0.0202845,0.994655,8.41465100921,0.000661076,115,0.0014176,0.000839233,0.47635,1.0,47.0,-0.000456445,36.0,-0.264625,-0.0692509,0.000779262,0.00480335,-0.00716521,115,1000,0.000299475,6.33712008006,0,-0.919762,0.000467844,3.0,45.5,-0.0043333,0.481168,1.0,0.000958165,-0.648149,0.000401434,29.836081818677485,58.0,0
