VF Loss,QF Loss,Test Actions Mean,Policy mu Max,Test Rewards Max,Test Rewards Min,Q Predictions Min,V Predictions Mean,Test Actions Std,Policy mu Mean,Policy log std Min,Exploration Actions Std,Test Rewards Std,Policy mu Std,Test Returns Mean,Sample Time (s),Log Pis Mean,Num Paths,Policy log std Mean,Q Predictions Max,Exploration Rewards Max,Test Actions Min,Test Returns Std,Epoch,Number of rollouts total,Log Pis Std,V Predictions Std,AverageReturn,Test Rewards Mean,Q Predictions Mean,Exploration Returns Max,Exploration Rewards Min,Exploration Rewards Mean,Q Predictions Std,Number of env steps total,Test Returns Max,V Predictions Max,Log Pis Min,Policy log std Std,Exploration Returns Std,Log Pis Max,Policy Loss,Exploration Returns Min,Exploration Actions Min,Total Train Time (s),Number of train steps total,Exploration Actions Mean,Train Time (s),Exploration Actions Max,Exploration Returns Mean,V Predictions Min,Policy log std Max,Policy mu Min,Test Actions Max,Epoch Time (s),(Previous) Eval Time (s),Test Returns Min,Exploration Rewards Std
0.428016,1.00295,-0.0530409,0.000630769,1.0,1.0,-0.00391897,-0.00158887,9.16018e-05,0.000448652,0.000663378,0.59818,0.0,0.000160407,8.74782608696,0.699306471273303,-0.617321,179,0.000773123,-0.00194269,1.0,-0.0532399,0.434260556146,0,179,0.22023,0.00144378,8.74782608696,1.0,-0.00299295,22.0,1.0,1.0,0.000554426,1000,9.0,0.000308422,-0.919738,2.98969e-05,3.23884578687,-0.263547,0.428798,3.0,-0.986786,52.87878724467009,3492,-0.00636966,51.57140824198723,0.993018,5.57541899441,-0.00584381,0.000822622,4.57294e-05,-0.0528994,52.27071471326053,0,8.0,0.0
