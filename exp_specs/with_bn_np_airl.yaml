meta_data:
  script_path: /h/kamyar/oorl_rlkit/run_scripts/fetch_custom_train_np_airl.py
  exp_dirs: /scratch/gobi2/kamyar/oorl_rlkit/output/
  exp_name: lower_disc_lr_no_clip_np_airl_another_32_demos_16_each_sub_8_pol_adam_0p9_z_dim_25_rew_2_and_4_disc_128
  description: searching over the SAC hyperparameters
  use_gpu: true
  num_workers: 4
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:

  algo_params:
    wrap_absorbing: [false]
    state_only: [false]

    # results in training batch size of 1024 for disc model
    num_tasks_used_per_update: [16]
    num_context_trajs_for_training: [3]
    num_test_trajs_for_training: [5]
    disc_samples_per_traj: [8]

    num_context_trajs_for_exploration: [3]

    num_tasks_per_eval: [10]
    num_diff_context_per_eval_task: [2]
    num_eval_trajs_per_post_sample: [2]
    num_context_trajs_for_eval: [3]

    policy_optim_batch_mode_random: [true]
    policy_optim_batch_size_per_task: [64] # results in batch size 1024
    policy_optim_batch_size_per_task_from_expert: [0]

    # encoder_lr: [0.0002]
    # disc_lr: [0.0002]
    encoder_lr: [0.00005]
    disc_lr: [0.00005]

    num_update_loops_per_train_call: [65]
    num_disc_updates_per_loop_iter: [1]
    num_policy_updates_per_loop_iter: [1]

    use_grad_pen: [true]
    # grad_pen_weight: [2.5, 5.0, 10.0]
    # grad_pen_weight: [2.5, 5.0]
    # grad_pen_weight: [5.0]
    # grad_pen_weight: [5.0]
    grad_pen_weight: [0.5]

    disc_ce_grad_clip: [3.0]
    enc_ce_grad_clip: [1.0]
    disc_gp_grad_clip: [1.0]
    # disc_ce_grad_clip: [100.0]
    # enc_ce_grad_clip: [100.0]
    # disc_gp_grad_clip: [100.0]

    use_target_disc: [true]
    soft_target_disc_tau: [0.005]

    use_target_enc: [true]
    soft_target_enc_tau: [0.005]

    np_params:
      z_dim: [25]
      agg_type: ['sum']
  
  policy_params:
    reward_scale: [2.0, 4.0]
    # reward_scale: [2.0, 4.0]
    # reward_scale: [5.0]
    # reward_scale: [10.0]
    # reward_scale: [2.0, 8.0]
    # reward_scale: [2.0, 4.0, 8.0, 10.0]
    use_policy_as_ema_policy: [true]
    soft_ema_policy_exp: [0.005]

  disc_clamp_magnitude: [10.0]

  policy_net_size: [256]
  num_hidden_layers: [3]

  env_specs:
    normalized: [false]
    train_test_env: [false]
  
  seed: [9783, 5914]

# -----------------------------------------------------------------------------
constants:
  expert_name: another_scale_0p9_new_gen_few_shot_fetch_linear_demos_32_tasks_16_total_each_subsample_8
  # expert_name: scale_0p9_new_gen_few_shot_fetch_linear_demos_12_tasks_10_total_each_subsample_8
  # expert_name: scale_0p9_linear_demos_50_tasks_25_each
  expert_seed_run_idx: 0
  
  algo_params:
    eval_deterministic: false
    
    num_epochs: 1011
    num_rollouts_per_epoch: 125 # num rollouts between updates * 25
    num_rollouts_between_updates: 5
    num_initial_rollouts_for_all_train_tasks: 5
    min_rollouts_before_training: 0
    max_path_length: 65

    policy_uses_pixels: false

    replay_buffer_size_per_task: 20000

    save_replay_buffer: true
    render: false

    freq_saving: 25

    np_params: {}
  
  # bad naming but policy_net_size and num_hidden_layers are also
  # for the value function and the Q function
  policy_params:
    discount: 0.99
    policy_lr: 0.00005
    qf_lr: 0.00005
    vf_lr: 0.00005
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    soft_target_tau: 0.005
    
  env_specs:
    base_env_name: 'scale_0p9_linear_demos_32_tasks'
    # base_env_name: 'scale_0p9_linear_demos_12_tasks'
    # base_env_name: 'scale_0p9_linear_demos_50_tasks_25_each'
    need_pixels: false
