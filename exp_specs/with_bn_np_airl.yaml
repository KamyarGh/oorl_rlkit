meta_data:
  script_path: run_scripts/fetch_custom_train_np_airl.py
  # exp_name: double_grad_clip_thresh_less_epochs_final_better_np_airl_KL_0p1_weight_share_enc_32_dim_pol_dim_64_gate_dim_32_z_dim_16
  # exp_name: fetch_np_airl_KL_0p001_epoch_60_to_120_few_shot_1_to_6_fucking_fixed_no_clip_DIM_24_all_clip_5
  # exp_name: fetch_np_airl_KL_0p001_DIM_24_with_target_enc_with_enc_grad_clip
  # exp_name: fetch_np_airl_KL_0p001_DIM_24_with_target_enc_with_enc_grad_clip
  exp_name: fetch_np_airl_no_KL_DIM_24_no_clip_1_to_3
  description: searching over the SAC hyperparameters
  use_gpu: true
  num_workers: 4
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  meta_fairl: [false]
  algo_params:
    save_best: [true]
    save_best_after_epoch: [40]
    # save_best_after_epoch: [120]
    # custom_save_epoch: [[60,120]]
    
    max_KL_beta: [0.0]
    KL_ramp_up_start_iter: [97500] # start at epoch 60
    KL_ramp_up_end_iter: [195000] # when you change this also change best after epoch

    wrap_absorbing: [false]
    state_only: [false]

    transfer_version: [false]

    few_shot_version: [true]
    max_context_size: [3]
    min_context_size: [1]

    # results in training batch size of 1024 for disc model
    num_tasks_used_per_update: [16]
    num_context_trajs_for_training: [3]
    num_test_trajs_for_training: [6]
    disc_samples_per_traj: [8]

    num_context_trajs_for_exploration: [3]

    num_tasks_per_eval: [10]
    num_diff_context_per_eval_task: [6]
    num_eval_trajs_per_post_sample: [3]
    num_context_trajs_for_eval: [3]

    policy_optim_batch_mode_random: [true]
    policy_optim_batch_size_per_task: [64] # results in batch size 1024
    policy_optim_batch_size_per_task_from_expert: [8]

    encoder_lr: [0.0003]
    disc_lr: [0.0003]
    # encoder_lr: [0.0002]
    # disc_lr: [0.0002]
    disc_Adam_beta: [0.0]
    r_V_reg_weight: [0.0]
    # encoder_lr: [0.00005]
    # disc_lr: [0.00005]

    num_update_loops_per_train_call: [65]
    num_disc_updates_per_loop_iter: [1]
    num_policy_updates_per_loop_iter: [1]

    use_grad_pen: [true]
    # grad_pen_weight: [2.5, 5.0, 10.0]
    # grad_pen_weight: [2.5, 5.0]
    # grad_pen_weight: [5.0]
    # grad_pen_weight: [5.0]
    grad_pen_weight: [0.5]

    # disc_ce_grad_clip: [8.0]
    # enc_ce_grad_clip: [2.0]
    # disc_gp_grad_clip: [2.0]
    disc_ce_grad_clip: [100.0]
    enc_ce_grad_clip: [100.0]
    disc_gp_grad_clip: [100.0]
    # disc_ce_grad_clip: [5.0]
    # enc_ce_grad_clip: [5.0]
    # disc_gp_grad_clip: [5.0]

    use_target_disc: [false]
    soft_target_disc_tau: [0.005]

    use_target_enc: [true]
    soft_target_enc_tau: [0.005]

    np_params:
      z_dim: [16]
      agg_type: ['sum']
  
  policy_params:
    reward_scale: [4.0]
    # reward_scale: [6.0, 8.0]
    # reward_scale: [10.0, 12.0]
    # reward_scale: [14.0, 16.0]
    # reward_scale: [2.0, 4.0]
    # reward_scale: [5.0]
    # reward_scale: [10.0]
    # reward_scale: [2.0, 8.0]
    # reward_scale: [2.0, 4.0, 8.0, 10.0]
    use_policy_as_ema_policy: [false]
    soft_ema_policy_exp: [0.005]

  disc_clamp_magnitude: [10.0]

  policy_net_size: [64]
  num_hidden_layers: [3]

  env_specs:
    normalized: [false]
    train_test_env: [false]
  
  # seed: [9783, 5914]
  seed: [1553, 7972, 9361, 1901]

# -----------------------------------------------------------------------------
constants:
  expert_name: fixed_colors_0p5_32_16_1
  # expert_name: another_scale_0p9_new_gen_few_shot_fetch_linear_demos_32_tasks_16_total_each_subsample_1
  # expert_name: another_scale_0p9_new_gen_few_shot_fetch_linear_demos_16_tasks_16_total_each_subsample_8
  # expert_name: another_scale_0p9_new_gen_few_shot_fetch_linear_demos_20_tasks_16_total_each_subsample_8
  # expert_name: another_scale_0p9_new_gen_few_shot_fetch_linear_demos_32_tasks_16_total_each_subsample_8
  # expert_name: scale_0p9_new_gen_few_shot_fetch_linear_demos_12_tasks_10_total_each_subsample_8
  # expert_name: scale_0p9_linear_demos_50_tasks_25_each
  expert_seed_run_idx: 0
  
  algo_params:
    eval_deterministic: false
    
    num_epochs: 161
    num_rollouts_per_epoch: 125 # num rollouts between updates * 25
    num_rollouts_between_updates: 5
    num_initial_rollouts_for_all_train_tasks: 5
    min_rollouts_before_training: 0
    max_path_length: 65

    policy_uses_pixels: false

    replay_buffer_size_per_task: 20000

    save_replay_buffer: false
    save_algorithm: true
    render: false

    freq_saving: 10

    np_params: {}
  
  # bad naming but policy_net_size and num_hidden_layers are also
  # for the value function and the Q function
  policy_params:
    discount: 0.99
    # policy_lr: 0.00005
    # qf_lr: 0.00005
    # vf_lr: 0.00005
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    soft_target_tau: 0.005
    
  env_specs:
    # base_env_name: 'scale_0p9_linear_demos_16_tasks'
    # base_env_name: 'scale_0p9_linear_demos_20_tasks'
    base_env_name: 'scale_0p9_linear_demos_32_tasks'
    # base_env_name: 'scale_0p9_linear_demos_12_tasks'
    # base_env_name: 'scale_0p9_linear_demos_50_tasks_25_each'
    need_pixels: false
