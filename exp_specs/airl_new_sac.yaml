meta_data:
  script_path: run_scripts/airl_exp_script.py
  # exp_name: final_correct_halfcheetah_airl_with_exp_reward_hyper_search_rew_scale_more_than_40
  # correct_normalized_halfcheetah_state_action_forw_KL_airl_disc
  # normalized_halfcheetah_state_action_forw_KL_airl_disc_rew_scale_search
  # exp_name: totally_absolutely_final_normalized_ant_forw_KL_airl_disc_final_state_only
  # exp_name: correct_no_grad_clip_halfcheetah_rev_KL
  # exp_name: correct_no_norm_no_save_best_rev_KL_ant_no_grad_clip
  # exp_name: 10x_steps_per_epoch_forw_KL_disc_hid_256
  # exp_name: little_data_forw_KL_disc_hid_64
  # exp_name: what_matters_halfcheetah_forw_KL_with_128_disc_100_rew
  # exp_name: halfcheetah_forw_KL_16_demos_20_sub_with_128_disc_100_rew_clipping_at_1_and_some_hype_search
  # exp_name: actually_ant_gail_rew_search
  # exp_name: airl_hc_gp_and_rew_search_16_demos_20_sub_correct_higher_rewards
  # exp_name: fairl_hopper_gp_and_rew_search_16_demos_sub_20_final_hype_search
  # exp_name: paper_version_fairl_4_walker_demos_correct_final_with_saving
  # exp_name: fairl_final_humanoid_hype_search_no_save_correct_final
  # exp_name: airl_final_ant_hype_search_rew_12
  # exp_name: fairl_humanoid_hype_search
  # exp_name: star_8_pedagogical_example_fairl_clip_min_neg_2p5_pol_64_2_disc_128_4_relu_bn
  exp_name: star_8_pedagogical_example_gail_shorter_epochs
  description: searching over the SAC hyperparameters
  use_gpu: true
  # use_gpu: false
  num_workers: 4
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  scale_env_with_given_demo_stats: [true]
  
  disc_num_blocks: [4]
  # disc_hid_dim: [64]
  # disc_hid_dim: [256]
  disc_hid_dim: [128]
  # disc_hid_dim: [32, 48]
  # disc_hid_dim: [48, 64, 128]
  # disc_hid_act: [tanh]
  disc_hid_act: [relu]
  # disc_use_bn: [false]
  disc_use_bn: [true]

  algo_params:
    # rew_clip_min: [-10]
    # rew_clip_min: [-2.5, -5.0]
    # rew_clip_min: [-2.5]

    save_best: [false]
    save_best_starting_from_epoch: [0]

    use_target_disc: [false]
    
    state_only: [false]

    # use_exp_rewards: [true]
    use_exp_rewards: [false]
    gail_mode: [true]
    # gail_mode: [false]

    # rew_clip_max: [1.0]
    # rew_clip_min: [-1.0, -2.0, -4.0, -8.0]

    no_terminal: [true]
    # replay_buffer_size: [15000]
    replay_buffer_size: [16000]

    num_update_loops_per_train_call: [1]
    # num_update_loops_per_train_call: [1000, 100]
    num_disc_updates_per_loop_iter: [20]
    num_policy_updates_per_loop_iter: [5]

    disc_optim_batch_size: [512]

    # use_grad_pen: [true, false]
    use_grad_pen: [true]
    # grad_pen_weight: [10.0]
    # grad_pen_weight: [0.5, 1.0, 2.0, 4.0, 8.0]
    # grad_pen_weight: [0.05, 0.1, 0.5]
    # grad_pen_weight: [0.1, 0.5, 1.0]
    grad_pen_weight: [0.5]
    # grad_pen_weight: [0.01, 0.05, 0.1, 0.5]
    # grad_pen_weight: [0.05, 0.01]
    # grad_pen_weight: [0.05, 0.1]
    # grad_pen_weight: [0.5, 2.0]
    # grad_pen_weight: [4.0, 8.0, 12.0, 16.0]
    # grad_pen_weight: [1.0, 2.0, 4.0]
    # grad_pen_weight: [10.0]
    # grad_pen_weight: [1.0, 5.0, 10.0]

    disc_ce_grad_clip: [100.0]
    disc_gp_grad_clip: [100.0]
    # disc_ce_grad_clip: [1.0]
    # disc_gp_grad_clip: [1.0]
  
  policy_params:
    # reward_scale: [15.0, 25.0, 40.0]
    # reward_scale: [10.0]
    # reward_scale: [8.0]
    # reward_scale: [50.0]
    # reward_scale: [100.0]
    # reward_scale: [25.0, 50.0, 75.0]
    # reward_scale: [8.0, 15.0, 100.0]
    # reward_scale: [125.0]
    # reward_scale: [18.0, 25.0, 50.0]
    # reward_scale: [8.0, 12.0]
    # reward_scale: [25.0]
    # reward_scale: [8.0, 12.0, 25.0, 50.0, 75.0, 100.0]
    # reward_scale: [8.0, 12.0, 25.0, 50.0, 75.0, 100.0, 150.0, 200.0]
    # reward_scale: [5.0, 10.0, 25.0]
    # reward_scale: [1.0, 2.0, 4.0]
    # reward_scale: [50.0, 100.0]
    # reward_scale: [50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 125.0, 150.0, 175.0, 200.0]
    # reward_scale: [0.0001, 0.001, 0.01, 0.1, 1.0, 5.0, 10.0]
    # reward_scale: [2.0, 4.0, 8.0, 12.0]
    # reward_scale: [2.0, 4.0, 8.0]
    # reward_scale: [1.0, 2.0, 4.0]
    # reward_scale: [2.0, 4.0, 8.0, 16.0, 32.0, 64.0, 128.0]
    # reward_scale: [2.0, 4.0, 8.0]
    # reward_scale: [16.0, 32.0, 64.0]
    # reward_scale: [64.0, 128.0, 192.0, 256.0]
    # reward_scale: [64.0, 128.0]
    # reward_scale: [64.0, 128.0]
    # reward_scale: [4.0, 8.0, 12.0, 16.0]
    # reward_scale: [4.0, 8.0]
    # reward_scale: [12.0]
    # reward_scale: [8.0, 16.0, 32.0]
    # reward_scale: [1.0, 2.0, 4.0, 8.0, 16.0, 32.0]
    # reward_scale: [1.0, 2.0, 4.0, 8.0, 16.0, 32.0, 48.0, 64.0]
    reward_scale: [1.0, 2.0, 4.0, 8.0]
    # reward_scale: [16.0]
    # reward_scale: [32.0, 64.0, 128.0]
    # reward_scale: [64.0]
    # reward_scale: [4.0]
    beta_1: [0.25]
    # beta_1: [0.0, 0.25, 0.5, 0.9]
  
  env_specs:
    normalized: [false]
  
  # seed: [9783, 5914, 4865, 2135, 2349]
  expert_seed_run_idx: [0]
  # seed: [9783, 5914]
  seed: [9783]
  # seed: [9783, 5914, 4865, 6601]
  # seed: [9783, 5914, 4865]

  expert_name: [
    # norm_multi_dir_point_mass_301_demos_ep_len_50
    norm_star_8_point_mass_128_demos_ep_len_25


    # halfcheetah_256_demos_20_subsampling,
    # halfcheetah_128_demos_20_subsampling,
    # halfcheetah_64_demos_20_subsampling,
    # halfcheetah_32_demos_20_subsampling,
    # halfcheetah_16_demos_20_subsampling,
    # halfcheetah_8_demos_20_subsampling,
    # halfcheetah_4_demos_20_subsampling,
    # halfcheetah_250_demos_no_subsampling,

    # norm_halfcheetah_256_demos_20_subsampling,
    # norm_halfcheetah_128_demos_20_subsampling,
    # norm_halfcheetah_64_demos_20_subsampling,
    # norm_halfcheetah_32_demos_20_subsampling,
    # norm_halfcheetah_16_demos_20_subsampling,
    # norm_halfcheetah_8_demos_20_subsampling,
    # norm_halfcheetah_4_demos_20_subsampling,
    # normalized_halfcheetah_250_demos_no_subsampling,

    # ant_256_demos_20_subsampling,
    # ant_128_demos_20_subsampling,
    # ant_64_demos_20_subsampling,
    # ant_32_demos_20_subsampling,
    # ant_16_demos_20_subsampling,
    # ant_8_demos_20_subsampling,
    # ant_4_demos_20_subsampling,

    # norm_ant_256_demos_20_subsampling,
    # norm_ant_128_demos_20_subsampling,
    # norm_ant_64_demos_20_subsampling,
    # norm_ant_32_demos_20_subsampling,
    # norm_ant_16_demos_20_subsampling,
    # norm_ant_8_demos_20_subsampling,
    # norm_ant_4_demos_20_subsampling,

    # norm_new_humanoid_128_demos_20_subsampling,
    # norm_new_humanoid_64_demos_20_subsampling,
    
    # norm_humanoid_128_demos_20_subsampling,
    # norm_humanoid_192_demos_20_subsampling,
    # norm_humanoid_256_demos_20_subsampling


    # norm_walker_32_demos_20_subsampling,
    # norm_walker_16_demos_20_subsampling,
    # norm_walker_4_demos_20_subsampling,

    # norm_hopper_32_demos_20_subsampling,
    # norm_hopper_16_demos_20_subsampling,
    # norm_hopper_4_demos_20_subsampling,


  ]
# -----------------------------------------------------------------------------
constants:
  # expert_name: halfcheetah_25_demos_subsample_20
  wrap_absorbing_state: false
  
  disc_clamp_magnitude: 10.0

  algo_params:
    num_epochs: 201
    # num_steps_per_epoch: 160000
    num_steps_per_epoch: 16000
    num_steps_per_eval: 3200
    num_steps_between_updates: 1600
    max_path_length: 25
    min_steps_before_training: 3200

    traj_based: false

    # disc_optim_batch_size: 1024
    policy_optim_batch_size: 512
    policy_optim_batch_size_from_expert: 0

    # use_target_disc: true
    soft_target_disc_tau: 0.005
    use_disc_input_noise: false

    policy_uses_task_params: false
    concat_task_params_to_policy_obs: false
    policy_uses_pixels: false
    
    save_replay_buffer: false
    render: false

    disc_lr: 0.0003
    disc_momentum: 0.0
    # use_grad_pen: true

    freq_saving: 20
  
  policy_net_size: 64
  # policy_num_hidden_layers: 4
  policy_num_hidden_layers: 2
  policy_params:
    discount: 0.99
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    soft_target_tau: 0.005

    
  env_specs:
    base_env_name: vanilla_point_mass_env
    # base_env_name: 'humanoid_v2'
    # base_env_name: 'ant_v2'
    # base_env_name: 'walker_v2'
    # base_env_name: 'hopper_v2'
    # base_env_name: 'halfcheetah_v2'
    train_test_env: false
