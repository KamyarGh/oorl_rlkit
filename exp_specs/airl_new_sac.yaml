meta_data:
  script_path: /h/kamyar/oorl_rlkit/run_scripts/airl_exp_script.py
  exp_dirs: /scratch/gobi2/kamyar/oorl_rlkit/output
  # exp_name: final_correct_halfcheetah_airl_with_exp_reward_hyper_search_rew_scale_more_than_40
  exp_name: forw_KL_hype_search_disc_with_bn
  description: searching over the SAC hyperparameters
  use_gpu: false
  num_workers: 6
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  disc_num_blocks: [2]
  disc_hid_dim: [48]
  # disc_hid_dim: [48, 64, 128]
  disc_hid_act: [tanh]
  disc_use_bn: [true]

  algo_params:
    use_exp_rewards: [true]
    # rew_clip_max: [1.0]
    # rew_clip_min: [-1.0, -2.0, -4.0, -8.0]

    no_terminal: [true]
    replay_buffer_size: [1000000]

    num_update_loops_per_train_call: [100]
    # num_update_loops_per_train_call: [1000, 100]
    num_disc_updates_per_loop_iter: [1]
    num_policy_updates_per_loop_iter: [1]

    disc_optim_batch_size: [256]
    grad_pen_weight: [10.0]
    # grad_pen_weight: [1.0, 5.0, 10.0]
    disc_ce_grad_clip: [1.0]
    disc_gp_grad_clip: [1.0]
  
  policy_params:
    reward_scale: [50.0, 75.0, 100.0]
    # reward_scale: [50.0, 60.0, 70.0, 80.0, 90.0, 100.0, 125.0, 150.0, 175.0, 200.0]
    # reward_scale: [0.0001, 0.001, 0.01, 0.1, 1.0, 5.0, 10.0]
    # reward_scale: [2.0, 4.0, 8.0, 12.0]
    # reward_scale: [2.0, 4.0, 8.0]
    # reward_scale: [1.0, 2.0, 4.0]
    beta_1: [0.25]
    # beta_1: [0.0, 0.25, 0.5, 0.9]
  
  env_specs:
    normalized: [false]
  
  # seed: [9783, 5914, 4865, 2135, 2349]
  expert_seed_run_idx: [0]
  seed: [9783, 5914]

# -----------------------------------------------------------------------------
constants:
  expert_name: halfcheetah_250_demos_no_subsampling
  # expert_name: halfcheetah_25_demos_subsample_20
  wrap_absorbing_state: false
  
  disc_clamp_magnitude: 10.0

  algo_params:
    num_epochs: 3011
    num_steps_per_epoch: 10000
    num_steps_per_eval: 10000
    num_steps_between_updates: 1000
    max_path_length: 1000
    min_steps_before_training: 5000


    traj_based: false
    state_only: false

    # disc_optim_batch_size: 1024
    policy_optim_batch_size: 256
    policy_optim_batch_size_from_expert: 0

    use_target_disc: false
    use_disc_input_noise: false

    policy_uses_task_params: false
    concat_task_params_to_policy_obs: false
    policy_uses_pixels: false

    
    save_replay_buffer: false
    render: false

    disc_lr: 0.0003
    disc_momentum: 0.0
    use_grad_pen: true

    freq_saving: 10
  
  policy_net_size: 256
  policy_num_hidden_layers: 2
  policy_params:
    discount: 0.99
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    soft_target_tau: 0.005

    
  env_specs:
    base_env_name: 'halfcheetah_v2'
    train_test_env: false
