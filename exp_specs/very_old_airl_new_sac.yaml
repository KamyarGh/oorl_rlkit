meta_data:
  script_path: /u/kamyar/oorl_rlkit/run_scripts/airl_exp_script.py
  exp_dirs: /ais/gobi6/kamyar/oorl_rlkit/output
  exp_name: airl_pendulum_grad_pen_using_log_prob_f_hid_32_direct_f
  description: searching over the SAC hyperparameters
  num_workers: 1
  cpu_range: [3,4]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  concat_env_params_to_obs: [false]

  # airl_params:

  # policy_params:
    # reward_scale: [5.0] there is not need for a reward scale hopefully
    # soft_target_tau: [0.005]
  
  env_specs:
    normalized: [true]
  
  seed: [9783]

# -----------------------------------------------------------------------------
constants:
  expert_save_dict: '/ais/gobi6/kamyar/oorl_rlkit/output/final-pendulum-new-sac-hyper-param-search/final_pendulum_new_sac_hyper_param_search_2018_10_12_19_11_09_0000--s-0/extra_data.pkl'
  last_k_expert_steps: 500
  
  airl_params:
    num_epochs: 501
    num_steps_per_epoch: 10000
    num_steps_per_eval: 10000
    max_path_length: 100

    num_reward_updates: 3
    num_policy_updates: 32
    rewardf_optim_batch_size: 1000
    policy_optim_batch_size: 32

    # the interaction between these two numbers matters
    num_steps_between_updates: 1000
    replay_buffer_size: 1000 # this is the buffer for storing the policy rollouts
    min_steps_before_training: 1000
    
    save_replay_buffer: true
    render: false

    rewardf_lr: 0.0003
    use_grad_pen: true
    grad_pen_weight: 10.0
  
  # rewardf_model_name: generic_reward_f
  # rewardf_model_specs:
    # hid_dim: 32
    # num_layers: 2

  policy_net_size: 256
  policy_params:
    # policy always has 2 hidden layers

    reward_scale: 1.0
    discount: 0.99
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    soft_target_tau: 0.005

    
  env_specs:
    base_env_name: 'pendulum_v0'
