meta_data:
  script_path: /u/kamyar/oorl_rlkit/run_scripts/gail_exp_script.py
  exp_dirs: /ais/gobi6/kamyar/oorl_rlkit/output
  exp_name: temp_dac_dmcs_simple_meta_reacher_2_layer_disc_batch_norm
  description: searching over the SAC hyperparameters
  use_gpu: false
  num_workers: 24
  cpu_range: [0,24]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  concat_env_params_to_obs: [false]

  num_expert_trajs: [100, 25]

  gail_params:
    no_terminal: [false]
    replay_buffer_size: [100000]

    num_reward_updates: [100]
    num_policy_updates: [100]
  
  policy_params:
    # reward_scale: [10.0]
    reward_scale: [1.0, 5.0, 10.0, 25.0, 50.0, 100.0]
    # soft_target_tau: [0.005]
  
  env_specs:
    normalized: [false]
    train_test_env: [true]
  
  seed: [9783, 5914]
  # seed: [9783]

# -----------------------------------------------------------------------------
constants:
  # expert_save_dict: '/ais/gobi6/kamyar/oorl_rlkit/output/final-pendulum-new-sac-hyper-param-search/final_pendulum_new_sac_hyper_param_search_2018_10_12_19_11_09_0000--s-0/extra_data.pkl'
  # last_k_expert_steps: 500
  expert_name: temp_dmcs_with_finger_pos_and_sin_cos_simple_meta_reacher_100_trajs_1_subsampling
  expert_seed_run_idx: 4
  wrap_absorbing_state: false
  
  gail_params:
    num_epochs: 511
    num_steps_per_epoch: 1000
    num_steps_per_eval: 1000
    max_path_length: 100

    # num_reward_updates: 3
    # num_policy_updates: 32
    disc_optim_batch_size: 100
    policy_optim_batch_size: 100

    policy_uses_pixels: false
    policy_uses_task_params: true
    concat_task_params_to_policy_obs: true

    # the interaction between these two numbers matters
    num_steps_between_updates: 100
    # replay_buffer_size: 1000 # this is the buffer for storing the policy rollouts
    min_steps_before_training: 1000
    
    save_replay_buffer: true
    render: false

    disc_lr: 0.0003
    use_grad_pen: true
    grad_pen_weight: 10

    freq_saving: 100
  
  policy_net_size: 256
  policy_params:
    # policy always has 2 hidden layers
    discount: 0.99
    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001
    soft_target_tau: 0.005

    
  env_specs:
    base_env_name: 'dmcs_simple_meta_reacher'
