meta_data:
  script_path: run_scripts/np_bc_exp_script.py
  # exp_name: hc_rand_vel_unnorm_16_demos_sub_20
  # exp_name: hc_rand_vel_64_per_task_no_sub_unnorm_debug_1_to_2_values_beta_1_0p9_norm
  # exp_name: hc_rand_vel_64_demos_sub_1_no_norm_with_saving
  exp_name: hc_rand_vel_64_demos_sub_1_no_saving_new_version_hype_search_with_lr_0p02
  description: searching over the SAC hyperparameters
  use_gpu: true
  num_workers: 8
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  scale_env_with_given_demo_stats: [false]

  algo_params:
    save_best: [false]
    save_best_after_epoch: [0]

    # objective: ['mse']
    objective: ['max_like']
    max_KL_beta: [0.0]
    KL_ramp_up_start_iter: [20000]
    KL_ramp_up_end_iter: [50000]
    wrap_absorbing: [false]

    few_shot_version: [true]
    max_context_size: [4]
    min_context_size: [1]

    # results in training batch size of 1024 for disc model
    # num_tasks_used_per_update: [8]
    # num_context_trajs_for_training: [4]
    # num_test_trajs_for_training: [4]
    # train_samples_per_traj: [16]
    # for debugging --------------
    num_tasks_used_per_update: [8]
    num_context_trajs_for_training: [4] # does not matter
    num_test_trajs_for_training: [4]
    train_samples_per_traj: [16]
    policy_optim_batch_size_per_task: [128]

    num_context_trajs_for_exploration: [4]

    # num_tasks_per_eval: [24]
    # num_diff_context_per_eval_task: [2]
    # num_eval_trajs_per_post_sample: [1]
    # num_context_trajs_for_eval: [3]
    # for debugging ---------------
    num_tasks_per_eval: [5]
    num_diff_context_per_eval_task: [2]
    num_eval_trajs_per_post_sample: [3]
    num_context_trajs_for_eval: [3]

    # encoder_lr: [0.0003]
    # policy_lr: [0.0003]
    encoder_lr: [0.02]
    policy_lr: [0.02]
    # beta_1: [0.25]
    beta_1: [0.25, 0.9]
    # beta_1: [0.25]

    num_update_loops_per_train_call: [1000]

    use_target_enc: [false]
    soft_target_enc_tau: [0.005]

    use_target_policy: [false]
    soft_target_policy_tau: [0.005]

    r_dim: [64]
    z_dim: [64]
    enc_hid_dim: [256]
    r2z_hid_dim: [64]
    num_enc_layer_blocks: [2]
    agg_type: ['sum']
    within_traj_agg: ['mean']
    # --------
    # z_dim: [32]
    # np_params:
    #   traj_enc_params:
    #     num_conv_layers: [3]
    #     channels: [64]
    #     kernel: [3]
    #     stride: [3]
    #   Dc2r_params:
    #     agg_type: ['sum']
    #   r2z_params:
    #     num_layers: [1]
    #     hid_dim: [32]

  policy_net_size: [256]
  num_hidden_layers: [3]

  # seed: [7723, 4969, 3082, 2290]
  # seed: [7723]
  seed: [7723, 4969]
  expert_name: [
    hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1,
    norm_hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1,
    # hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1
    # hc_rand_vel_expert_demos_0p1_separated_4_demos_sub_20
  ]

# -----------------------------------------------------------------------------
constants:
  # expert_name: norm_hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_20
  # expert_name: hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_20
  # expert_name: norm_hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1
  # expert_name: norm_hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1_only_task_1
  # expert_name: norm_hc_rand_vel_expert_demos_0p1_separated_256_demos_sub_1_only_task_1
  # expert_name: hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1_only_task_1
  # expert_name: hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_20
  # expert_name: hc_rand_vel_expert_demos_0p1_separated_16_demos_sub_20
  # expert_name: hc_rand_vel_expert_demos_0p1_separated_64_demos_sub_1
  # expert_name: hc_rand_vel_expert_debug_9_values
  # expert_name: norm_hc_rand_vel_expert_debug_1_to_2_values

  expert_seed_run_idx: 0
  
  algo_params:
    eval_deterministic: true
    
    num_epochs: 101
    num_rollouts_per_epoch: 1
    num_rollouts_between_updates: 1
    num_initial_rollouts_for_all_train_tasks: 0
    min_rollouts_before_training: 0
    # max_path_length: 200
    max_path_length: 1000

    policy_uses_pixels: false

    replay_buffer_size_per_task: 2000

    save_replay_buffer: false
    save_algorithm: false
    render: false

    freq_saving: 5

    np_params:
      traj_enc_params: {}
      Dc2r_params: {}
      r2z_params: {}

  env_specs:
    # base_env_name: 'halfcheetah_rand_vel_0p1_separated'
    base_env_name: 'halfcheetah_rand_vel_0p1_separated'
    normalized: false
    train_test_env: false
    need_pixels: false
