meta_data:
  script_path: /h/kamyar/oorl_rlkit/run_scripts/train_np_airl.py
  exp_dirs: /scratch/gobi2/kamyar/oorl_rlkit/output
  exp_name: test_zero_z
  description: searching over the SAC hyperparameters
  use_gpu: false
  num_workers: 1
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  disc_hidden_sizes: [
    # [100, 64],

    [100, 100],
    # [256, 256],
    # [100, 100, 100],
    
    # [256, 256, 256],
  ]
  disc_uses_layer_norm: [false]
  
  algo_params:
    use_grad_pen: [true]
    grad_pen_weight: [1.0, 5.0, 10.0]

    policy_net_size: [100]
    policy_num_layers: [3]
    # policy_num_layers: [3, 4]
    policy_params:
      # reward_scale: [2.0, 4.0, 6.0, 8.0]
      reward_scale: [4.0, 6.0, 10.0, 20.0, 40.0, 80.0]

    num_disc_updates_per_epoch: [130]
    num_policy_updates_per_epoch: [65]

    num_tasks_used_per_update: [1]
    num_context_trajs_for_training: [3]
    num_test_trajs_for_training: [5]
    policy_batch_size_per_task: [1024]
  
    np_params:
      z_dim: [50]

      traj_enc_params:
          timestep_enc_params:
            # input_size must be inferred based on env
              [
                {hidden_sizes: [], output_size: 50},
                # {hidden_sizes: [100], output_size: 100}
                # {hidden_sizes: [256], output_size: 256}
              ]

  seed: [9783, 5914]
  # seed: [9783]
  # seed: [9783, 5914, 4865, 2135, 2349]

# -----------------------------------------------------------------------------
constants:
  expert_name: normalized_single_task_basic_few_shot_fetch_larger_object_range_expert_50_10_10
  expert_seed_run_idx: 0

  algo_params:
    num_epochs: 100011
    num_rollouts_per_epoch: 1
    num_initial_rollouts_for_all_train_tasks: 8
    min_rollouts_before_training: 0
    max_path_length: 65

    replay_buffer_size_per_task: 1000000
    no_terminal: false

    num_tasks_per_eval: 1
    num_diff_context_per_eval_task: 2
    num_context_trajs_for_eval: 3
    num_eval_trajs_per_post_sample: 10

    num_context_trajs_for_exploration: 3
    encoder_lr: 0.0003

    # policy params
    policy_uses_pixels: false
    policy_params:
      # policy always has 2 hidden layers
      discount: 0.99
      policy_lr: 0.0003
      qf_lr: 0.0003
      vf_lr: 0.0003
      policy_mean_reg_weight: 0.001
      policy_std_reg_weight: 0.001
      soft_target_tau: 0.005


    np_params:
      # z_dim: 20

      traj_enc_params:
      #   timestep_enc_params:
      #     # input_size must be inferred based on env
      #     hidden_sizes: [50]
      #     output_size: 50

        traj_enc_params:
          # input_size must be inferred
          hidden_sizes: [100]
          output_size: 50
      
      r2z_map_params:
        trunk_params:
          # input_size must be inferred
          hidden_sizes: []
          output_size: 50
        split_heads_params:
          # input_size must be inferred
          hidden_sizes: []
          # output_size must be inferred
      
      np_enc_params:
        agg_type: 'sum'
        

    save_replay_buffer: true
    save_algorithm: true
    render: false

    freq_saving: 250
    freq_eval: 25
    
  env_specs:
    base_env_name: 'debug_scaled_basic_few_shot_fetch_env'
    normalized: false
    need_pixels: false
