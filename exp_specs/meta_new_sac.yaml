meta_data:
  script_path: run_scripts/sac.py
  # exp_name: ant_90_deg_apart_max_path_300_l1_reward
  # exp_name: ant_quarter_circle_l1_rew_smal_ctrl_cost_no_contact_cost
  # exp_name: ant_original_model_large_rew_200_plus_survival_rew
  # exp_name: ant_original_model_large_squared_rew_plus_survival_rew_scale_150
  # exp_name: ant_original_model_large_squared_rew_plus_survival_middle_60_deg_rew_scale_200
  # exp_name: ant_large_expert_one_direction_pi_over_32_16_points_test_15_no_survive
  # exp_name: ant_expert_24_points_pol_512_3_roll_between_8_rew_scale_50
  # exp_name: ant_8_star_obs_uses_ant_cfrc_only
  # exp_name: ant_8_star_obs_uses_no_cfrc
  # exp_name: ant_random_direction_running_better_reward_function
  # exp_name: ant_rand_goal_r_20_90_to_135
  exp_name: ant_rand_goal_denser_rewards
  description: searching over the SAC hyperparameters
  use_gpu: true
  num_workers: 4
  cpu_range: [0,159]
  num_cpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  use_custom_ant_models: [false]
  goal_embed_dim: [32]
  algo_params:
    save_best: [true]
    save_best_after_epoch: [0]

    # reward_scale: [100.0, 200.0, 300.0, 400.0]
    # reward_scale: [25.0, 50.0, 100.0, 150.0, 200.0]
    # reward_scale: [1.0, 2.0, 5.0, 8.0, 10.0, 20.0, 40.0, 80.0]
    # reward_scale: [5.0, 10.0, 25.0, 50.0, 75.0, 100.0, 150.0, 200.0]
    # reward_scale: [50.0, 100.0, 150.0, 200.0]
    # reward_scale: [200.0]
    # reward_scale: [2.0, 4.0, 8.0, 12.0, 16.0, 20.0]
    # reward_scale: [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0]
    # reward_scale: [1.0, 2.5, 5.0, 10.0, 15.0]
    # reward_scale: [15.0, 10.0, 5.0, 2.5, 1.0]
    reward_scale: [10.0, 5.0]
    policy_uses_task_params: [true]
    policy_uses_pixels: [false]
  # seed: [1553]
  # seed: [1553, 7972, 9361, 1901]
  seed: [8734, 4082]
  # seed: [8734]

# -----------------------------------------------------------------------------
constants:
  algo_params:
    meta: true
    do_running_obs_norm: false

    num_epochs: 1000001
    num_rollouts_per_epoch: 160
    num_rollouts_between_updates: 32
    num_initial_rollouts_for_all_train_tasks: 125
    # num_initial_rollouts_for_all_train_tasks: 10
    num_tasks_per_eval: 32
    num_eval_trajs_per_task: 4
    num_updates_per_train_call: 500
    # num_updates_per_train_call: 250
    # max_path_length: 1000
    # max_path_length: 400
    # max_path_length: 500
    max_path_length: 100
    # max_path_length: 100
    # max_path_length: 500
    discount: 0.99
    # replay_buffer_size_per_task: 80000
    # replay_buffer_size_per_task: 500000
    # replay_buffer_size_per_task: 100000
    # replay_buffer_size_per_task: 25000
    replay_buffer_size_per_task: 64000
    # replay_buffer_size_per_task: 1000000
    # replay_buffer_size_per_task: 30000
    # replay_buffer_size_per_task: 250000
    # replay_buffer_size_per_task: 10000
    # replay_buffer_size_per_task: 25000

    num_tasks_per_batch: 8
    num_samples_per_task_per_batch: 128
    soft_target_tau: 0.005

    policy_lr: 0.0003
    qf_lr: 0.0003
    vf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001

    render: false
    save_algorithm: True
    freq_saving: 10

  net_size: 512
  # net_size: 256
  num_hidden_layers: 3
  # net_size: 100
  # num_hidden_layers: 2

  env_specs:
    # base_env_name: 'ant_8_star_env'
    # base_env_name: 'ant_rand_direc_running_45_to_135'

    # base_env_name: 'ant_rand_goal_r_20_45_to_90'
    # base_env_name: 'ant_rand_goal_r_20_90_to_135'

    base_env_name: 'ant_rand_goal_32_points'

    # base_env_name: 'ant_rand_goal_24_points'
    # base_env_name: 'ant_rand_goal_one_direction'
    # base_env_name: 'ant_rand_goal_45_deg_farther'
    # base_env_name: 'ant_rand_goal_opposite_2_directions'

    # base_env_name: 'ant_rand_goal_90_deg_apart'
    
    # base_env_name: 'ant_rand_goal_2_directions'
    # base_env_name: 'ant_rand_goal_60_degrees'

    # base_env_name: 'ant_rand_goal_middle_60'
    
    # base_env_name: 'ant_rand_goal_line_params_sampler'
    # base_env_name: 'ant_rand_goal_120_degrees'
    # base_env_name: 'ant_rand_goal_200_points'
    normalized: false
    train_test_env: false
    need_pixels: false
