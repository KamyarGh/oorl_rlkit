- separate rl algorithm from meta-rl algorithm
- document the parameters of GAIL
- change the name from GAIL to DAC
- merge fetch_custom with gail_exp script
- gen_meta_irl_expert_trajs.py
    - assuming the policy was trained with rl so it has an exploration policy
- init for envs
- policy can't simultaneously use pixels and non-pixel inputs
- make a subclass of gym Env for a meta env and make it so that it has to have things
    like iterators etc. and move every constraint in there
- working with both task_identifiers and task_params and obs_task_params is unintuitive and messy, fix it
- fix the monkey patching pixels.wrapper
- how replay buffer handles returning pixels. Right now if you return pixels you also return obs
- some functions in simple replay buffer are really old and not needed
- the bug in simple replay buffer due to size being same as max path length
- removing any trace of expert_replay_buffer
- in irl_algorithm num_steps_between_updates is a bit weird
- envs.__init__ is very inefficient (figure out how gym.envs.make works and make something like that)
- add the files for new and rotated no head fetch environments to the repo
- either remove or generalize copy over params
- some things say algo_params some things say policy params
- clean up ~/local_params and ~/oorl_rlkit/local_params
- I need to clean up wrapped_goal....
- documentation for the interface of meta-environments and task_samplers
- you really need to sort out the yaml files... the nesting structure makes no sense at this point
- in meta-irl-algorithm freq_eval stuff is ugly
- fix normalize meta-expert-demos and other normalization scripts
- need to rethink how meta tasks and meta task samplers are defined, their interfaces, etc.
- Fully Specific Params Sampler for Fetch has too much copy-paste
- make gen_meta_irl_expert_trajs more general
- clean up few_shot_fetch_env.py
- remove the Wrapped Absorbing Env wrapper

- need to remove the copy of wrapped_goal.....
